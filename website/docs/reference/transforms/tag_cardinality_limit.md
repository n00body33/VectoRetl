---
component_title: "Tag Cardinality Limit"
description: "The Vector `tag_cardinality_limit` transform accepts and outputs `metric` events allowing you to limit the cardinality of tags."
event_types: ["metric"]
issues_url: https://github.com/timberio/vector/issues?q=is%3Aopen+is%3Aissue+label%3A%22transform%3A+tag_cardinality_limit%22
min_version: null
service_name: "Tag Cardinality Limit"
sidebar_label: "tag_cardinality_limit|[\"metric\"]"
source_url: https://github.com/timberio/vector/tree/master/src/transforms/tag_cardinality_limit.rs
status: "prod-ready"
title: "Tag Cardinality Limit Transform"
---

The Vector `tag_cardinality_limit` transform accepts and outputs [`metric`][docs.data-model.metric] events allowing you to limit the cardinality of tags.

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/docs/reference/transforms/tag_cardinality_limit.md.erb
-->

## Configuration

import CodeHeader from '@site/src/components/CodeHeader';

<CodeHeader fileName="vector.toml" learnMoreUrl="/docs/setup/configuration/"/ >

```toml
[transforms.my_transform_id]
  # REQUIRED
  type = "tag_cardinality_limit" # must be: "tag_cardinality_limit"
  inputs = ["my-source-id"] # example
  mode = "exact" # example, enum

  # OPTIONAL
  cache_size_per_tag = 5120000 # default, bytes, relevant when mode = "probabilistic"
  limit_exceeded_action = "drop_tag" # default, enum
  value_limit = 500 # default
```


## Options

import Fields from '@site/src/components/Fields';

import Field from '@site/src/components/Field';

<Fields filters={true}>


<Field
  common={true}
  defaultValue={5120000}
  enumValues={null}
  examples={[5120000]}
  groups={[]}
  name={"cache_size_per_tag"}
  path={null}
  relevantWhen={{"mode":"probabilistic"}}
  required={false}
  templateable={false}
  type={"int"}
  unit={"bytes"}
  >

### cache_size_per_tag

The size of the cache in bytes to use to detect duplicate tags. The bigger the cache the less likely it is to have a 'false positive' or a case where we allow a new value for tag even after we have reached the configured limits. See [Memory Utilization](#memory-utilization) for more info.


</Field>


<Field
  common={true}
  defaultValue={"drop_tag"}
  enumValues={{"drop_tag":"Remove tags that would exceed the configured limit from the incoming metric","drop_event":"Drop any metric events that contain tags that would exceed the configured limit"}}
  examples={["drop_tag","drop_event"]}
  groups={[]}
  name={"limit_exceeded_action"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

### limit_exceeded_action

Controls what should happen when a metric comes in with a tag that would exceed the configured limit on cardinality.


</Field>


<Field
  common={true}
  defaultValue={null}
  enumValues={{"exact":"Has higher memory requirements than `probabilistic`, but never falsely outputs metrics with new tags after the limit has been hit.","probabilistic":"Has lower memory requirements than `exact`, but may occassionally allow metric events to pass through the transform even when they contain new tags that exceed the configured limit.  The rate at which this happens can be controlled by changing the value of [`cache_size_per_tag`](#cache_size_per_tag)."}}
  examples={["exact","probabilistic"]}
  groups={[]}
  name={"mode"}
  path={null}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  >

### mode

Controls what approach is used internally to keep track of previously seen tags and deterime when a tag on an incoming metric exceeds the limit.


</Field>


<Field
  common={true}
  defaultValue={500}
  enumValues={null}
  examples={[500]}
  groups={[]}
  name={"value_limit"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"int"}
  unit={null}
  >

### value_limit

How many distinct values to accept for any given key. See [Memory Utilization](#memory-utilization) for more info.


</Field>


</Fields>

## Output


## How It Works

### Environment Variables

Environment variables are supported through all of Vector's configuration.
Simply add `${MY_ENV_VAR}` in your Vector configuration file and the variable
will be replaced before being evaluated.

You can learn more in the [Environment Variables][docs.configuration#environment-variables]
section.

### Memory Utilization

This transform stores in memory a copy of the key for every tag on every metric
event seen by this transform.  In mode `exact`, a copy of every distinct
value *for each key* is also kept in memory, until [`value_limit`](#value_limit) distinct values
have been seen for a given key, at which point new values for that key will be
rejected.  So to estimate the memory usage of this transform in mode `exact`
you can use the following formula:
(number of distinct field names in the tags for your metrics * average length of
the field names for the tags) + (number of distinct field names in the tags of
your metrics * [`value_limit`](#value_limit) * average length of the values of tags for your
metrics)

In mode `probabilistic`, rather than storing all values seen for each key, each
distinct key has a bloom filter which can probabilistically determine whether
a given value has been seen for that key.  The formula for estimating memory
usage in mode `probabilistic` is:
(number of distinct field names in the tags for your metrics * average length of
the field names for the tags) + (number of distinct field names in the tags of
-your metrics * [`cache_size_per_tag`](#cache_size_per_tag))

The [`cache_size_per_tag`](#cache_size_per_tag) option controls the size of the bloom filter used
for storing the set of acceptable values for any single key. The larger the
bloom filter the lower the false positive rate, which in our case means the less
likely we are to allow a new tag value that would otherwise violate a
configured limit. If you want to know the exact false positive rate for a given
`cache_size_per_tag` and [`value_limit`](#value_limit), there are many free online bloom filter
calculators that can answer this. The formula is generally presented in terms of
'n', 'p', 'k', and 'm' where 'n' is the number of items in the filter
(`value_limit` in our case), 'p' is the probability of false positives (what we
want to solve for), 'k' is the number of hash functions used internally, and 'm'
is the number of bits in the bloom filter. You should be able to provide values
for just 'n' and 'm' and get back the value for 'p' with an optimal 'k' selected
for you.   Remember when converting from [`value_limit`](#value_limit) to the 'm' value to plug
into the calculator that [`value_limit`](#value_limit) is in bytes, and 'm' is often presented
in bits (1/8 of a byte).


[docs.configuration#environment-variables]: /docs/setup/configuration/#environment-variables
[docs.data-model.metric]: /docs/about/data-model/metric/
