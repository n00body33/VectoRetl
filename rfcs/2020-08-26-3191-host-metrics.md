# RFC 3191 - 2020-08-26 - Collecting host-based metrics

This RFC is to introduce a new metrics source to consume host-based metrics. The high level plan is to implement one (or more?) sources that collect CPU, disk, and memory metrics from Linux-based hosts.

## Scope

This RFC will cover:

- A new source for host-based metrics, specifically:
  - CPU
  - Memory
  - Disk
- Collection on Linux, Windows, OSX.

This RFC will not cover:

- Other host metrics.
- Other platforms

## Motivation

Users want to collect, transform, and forward metrics to better observe how their hosts are performing.

## Internal Proposal

Build a single source called `host_metrics` (name to be confirmed) to collect host/system level metrics.

I've found a number of possible Rust-based solutions for implementing this collection, cross-platform.

### Multi-OS

- https://lib.rs/crates/sys-info (Rust + C)
- https://github.com/myfreeweb/systemstat (pure Rust)

### Linux

- https://docs.rs/procfs/0.8.0/procfs/
- https://docs.rs/linux-stats/0.3.1/linux_stats/
- https://docs.rs/sysinfo/0.3.19/sysinfo/index.html

We'd use one of these to collect the following metrics:

- `cpu_seconds_total` by mode (idle, nice, system, user) per CPU. (counter)
- `disk_read_bytes_total` by disk (counter)
- `disk_read_errors_total` by disk (counter)
- `disk_read_retries_total` by disk (counter)
- `disk_read_sectors_total` by disk (counter)
- `disk_read_time_seconds_total` by disk (counter)
- `disk_reads_completed_total` by disk (counter)
- `disk_write_errors_total` by disk (counter)
- `disk_write_retries_total` by disk (counter)
- `disk_write_time_seconds_total` by disk (counter)
- `disk_writes_completed_total` by disk (counter)
- `disk_written_bytes_total` by disk (counter)
- `disk_written_sectors_total` by disk (counter)
- `filesystem_avail_bytes` labeled with device, filesystem type, and mountpoint (gauge)
- `filesystem_device_error` labeled with device, filesystem type, and mountpoint (gauge)
- `filesystem_total_file_nodes` labeled with device, filesystem type, and mountpoint (gauge)
- `filesystem_free_file_nodes` labeled with device, filesystem type, and mountpoint (gauge)
- `filesystem_free_bytes` labeled with device, filesystem type, and mountpoint (gauge)
- `filesystem_size_bytes` labeled with device, filesystem type, and mountpoint (gauge)
- `load1` (gauge)
- `load5` (gauge)
- `load15` (gauge)
- `memory_active_bytes` (gauge)
- `memory_compressed_bytes` (gauge)
- `memory_free_bytes` (gauge)
- `memory_inactive_bytes` (gauge)
- `memory_swap_total`_bytes` (gauge)
- `memory_swap_used_bytes` (gauge)
- `memory_swapped_in_bytes_total` (gauge)
- `memory_swapped_out_bytes_total` (gauge)
- `memory_total_bytes` (gauge)
- `memory_wired_bytes` (gauge)

Metrics will also be labeled with:

- `host`: the host name of the host being monitored.

## Doc-level Proposal

The following additional source configuration will be added:

```toml
[sources.my_source_id]
  type = "host_metrics" # required
  scrape_interval_secs = 15 # optional, default, seconds
  namespace = "aspace" # optional, default, namespace to put metrics under
```

## Rationale

CPU, Memory, and Disk are the basic building blocks of host-based monitoring. They are considered "table stakes" for most metrics-based monitoring of hosts. Additionally, if we do not support ingesting metrics from them, it is likely to push people to use another tool

As part of Vector's vision to be the "one tool" for ingesting and shipping
observability data, it makes sense to add as many sources as possible to reduce
the likelihood that a user will not be able to ingest metrics from their tools.

## Prior Art

- https://github.com/influxdata/telegraf/tree/master/plugins/inputs/cpu
- https://github.com/influxdata/telegraf/tree/master/plugins/inputs/mem
- https://github.com/influxdata/telegraf/tree/master/plugins/inputs/disk
- https://github.com/elastic/beats/tree/master/metricbeat
- https://github.com/prometheus/node_exporter

## Drawbacks

- Additional maintenance and integration testing burden of a new source

## Alternatives

### Having users run telegraf or Prom node exporter and using Vector's prometheus source to scrape it

We could not add the source directly to Vector and instead instruct users to run
Telegraf or Prometheus' node exporter and point Vector at the exposed Prometheus scrape endpoint. This would leverage the already supported inputs from those projects.

I decided against this as it would be in contrast with one of the listed
principles of Vector:

> One Tool. All Data. - One simple tool gets your logs, metrics, and traces
> (coming soon) from A to B.

[Vector
principles](https://vector.dev/docs/about/what-is-vector/#who-should-use-vector)

On the same page, it is mentioned that Vector should be a replacement for
Telegraf.

> You SHOULD use Vector to replace Logstash, Fluent*, Telegraf, Beats, or
> similar tools.

If users are already running Telegraf or Node Exporter though, they could opt for this path.

## Outstanding Questions

- One source or many? Should we have `host_metrics` or `cpu_metrics`, `mem_metrics`, `disk_metrics`, or `load_metrics`?

## Plan Of Attack

Incremental steps that execute this change. Generally this is in the form of:

- [ ] Submit a PR with the initial source implementation

## Future Work

- Extend source to collect additional system-level metrics.
- Identify additional potential platforms.
