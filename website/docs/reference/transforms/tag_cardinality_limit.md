---
component_title: "Tag Cardinality Limit"
description: "The Vector `tag_cardinality_limit` transform accepts and outputs `metric` events allowing you to limit the cardinality of tags."
event_types: ["metric"]
issues_url: https://github.com/timberio/vector/issues?q=is%3Aopen+is%3Aissue+label%3A%22transform%3A+tag_cardinality_limit%22
min_version: null
service_name: "Tag Cardinality Limit"
sidebar_label: "tag_cardinality_limit|[\"metric\"]"
source_url: https://github.com/timberio/vector/tree/master/src/transforms/tag_cardinality_limit.rs
status: "prod-ready"
title: "Tag Cardinality Limit Transform"
---

The Vector `tag_cardinality_limit` transform accepts and outputs [`metric`][docs.data-model.metric] events allowing you to limit the cardinality of tags.

<!--
     THIS FILE IS AUTOGENERATED!

     To make changes please edit the template located at:

     website/docs/reference/transforms/tag_cardinality_limit.md.erb
-->

## Configuration

import CodeHeader from '@site/src/components/CodeHeader';

<CodeHeader fileName="vector.toml" learnMoreUrl="/docs/setup/configuration/"/ >

```toml
[transforms.my_transform_id]
  # REQUIRED
  type = "tag_cardinality_limit" # must be: "tag_cardinality_limit"
  inputs = ["my-source-id"] # example
  mode = "hash_set" # example, enum

  # OPTIONAL
  false_positive_rate = 1.0e-05 # default, relevant when mode = "bloom_filter"
  limit_exceeded_action = "drop_tag" # default, enum
  value_limit = 500 # default
```


## Options

import Fields from '@site/src/components/Fields';

import Field from '@site/src/components/Field';

<Fields filters={true}>


<Field
  common={true}
  defaultValue={1.0e-05}
  enumValues={null}
  examples={[1.0e-05]}
  groups={[]}
  name={"false_positive_rate"}
  path={null}
  relevantWhen={{"mode":"bloom_filter"}}
  required={false}
  templateable={false}
  type={"float"}
  unit={null}
  >

### false_positive_rate

Controls how likely it is that a metric with a new tag after the limit has been reached isn't caught by the transform. Lower values make this less likely at the cost of higher memory usage. See [Memory Utilization](#memory-utilization) for more info.


</Field>


<Field
  common={true}
  defaultValue={"drop_tag"}
  enumValues={{"drop_tag":"Remove tags that would exceed the configured limit from the incoming metric","drop_event":"Drop any metric events that contain tags that would exceed the configured limit"}}
  examples={["drop_tag","drop_event"]}
  groups={[]}
  name={"limit_exceeded_action"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"string"}
  unit={null}
  >

### limit_exceeded_action

Controls what should happen when a metric comes in with a tag that would exceed the configured limit on cardinality.


</Field>


<Field
  common={true}
  defaultValue={null}
  enumValues={{"hash_set":"Uses a HashSet internally to keep track of previously seen tag values. Has higher memory requirements than `bloom_filter`, but never falsely outputs metrics with new tags after the limit has been hit.","bloom_filter":"Uses a BloomFilter internally to keep track of previously seen tag values. Has lower memory requirements than `hash_set`, but may occassionally allow metric events to pass through the transform even when they contain new tags that exceed the configured limit.  The rate at which this happens can be controlled by changing the value of [`false_positive_rate`](#false_positive_rate)."}}
  examples={["hash_set","bloom_filter"]}
  groups={[]}
  name={"mode"}
  path={null}
  relevantWhen={null}
  required={true}
  templateable={false}
  type={"string"}
  unit={null}
  >

### mode

Controls what approach is used internally to keep track of previously seen tags and deterime when a tag on an incoming metric exceeds the limit.


</Field>


<Field
  common={true}
  defaultValue={500}
  enumValues={null}
  examples={[500]}
  groups={[]}
  name={"value_limit"}
  path={null}
  relevantWhen={null}
  required={false}
  templateable={false}
  type={"int"}
  unit={null}
  >

### value_limit

How many distinct values to accept for any given key. See [Memory Utilization](#memory-utilization) for more info.


</Field>


</Fields>

## Output


## How It Works

### Environment Variables

Environment variables are supported through all of Vector's configuration.
Simply add `${MY_ENV_VAR}` in your Vector configuration file and the variable
will be replaced before being evaluated.

You can learn more in the [Environment Variables][docs.configuration#environment-variables]
section.

### Memory Utilization

This transform stores in memory a copy of the key for every tag on every metric
event seen by this transform.  In mode `hash_set`, a copy of every distinct
value *for each key* is also kept in memory, until [`value_limit`](#value_limit) distinct values
have been seen for a given key, at which point new values for that key will be
rejected.  So to estimate the memory usage of this transform in mode `hash_set`
you can use the following formula:
(number of distinct field names in the tags for your metrics * average length of
the field names for the tags) + (number of distinct field names in the tags of
your metrics * [`value_limit`](#value_limit) * average length of the values of tags for your
metrics)

In mode `bloom_filter`, rather than storing all values seen for each key, each
distinct key has a bloom filter which can probabilistically determine whether
a given value has been seen for that key.  The formula for estimating memory
usage in mode `bloom_filter` is:
(number of distinct field names in the tags for your metrics * average length of
the field names for the tags) + (number of distinct field names in the tags of
your metrics * size of each bloom filter)

The size of each bloom filter will be based on the configured [`value_limit`](#value_limit)
and [`false_positive_rate`](#false_positive_rate). Higher values for [`value_limit`](#value_limit) and lower values for
`false_positive_rate` result in larger bloom filters.  The exact formula for
calculating the size of the bloom filter is complex, but there are many
free bloom filter size calculators available online. The formula is generally
presented in terms of 'n', 'p', 'k', and 'm' where 'n' is the number of items
in the filter (`value_limit` in our case), 'p' is the probability of false
positives (`false_positive_rate` in our case), 'k' is the number of hash
functions used internally, and 'm' is the number of bits in the bloom filter.
You should be able to provide values for just 'n' and 'p' and get back the
value for 'm' with an optional 'k' selected for you.  The value of 'm' that
the calculator gives you will tell you the memory usage required by each
bloom filter in this transform.  Generally speaking this mode should require
much less memory than mode `hash_set`.


[docs.configuration#environment-variables]: /docs/setup/configuration/#environment-variables
[docs.data-model.metric]: /docs/about/data-model/metric/
