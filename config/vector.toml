#                                    __   __  __
#                                    \ \ / / / /
#                                     \ V / / /
#                                      \_/  \/
#
#                                    V E C T O R
#                                   Configuration
#
# ------------------------------------------------------------------------------
# Website: https://vector.dev
# Docs: https://vector.dev/docs
# Chat: https://chat.vector.dev
# ------------------------------------------------------------------------------

# Change this to use a non-default directory for Vector data storage:
# data_dir = "/var/lib/vector"

# Random Syslog-formatted logs
[sources.logs]
type = "file"
data_dir = "/Users/vladimir.zhuk/dd/vector"
include = [ "/Users/vladimir.zhuk/dd/vector/test.log" ]
ignore_older_secs=60000000
ignore_checkpoints=true
read_from = "beginning"

[transforms.preprocessing]
type = "remap"
inputs = ["logs"]
source = '''
., err = parse_json(.message)
.custom = {}
'''

[transforms.processing]
type = "pipelines"
inputs = ["preprocessing"]

[transforms.processing.logs]
order = [
    "kubernetes",
    "flagship",
    "nginx",
    "envoy",
    "portus",
    "redis",
    "consul",
    "python",
    "rabbitmq",
    "vpa_xx",
    "zookeeper",
    "elasticsearch",
    "kafka",
    "proxysql",
    "ingestitron",
    "couchdb",
    "docker",
    "journald",
    "calico_parser",
    "requeue",
    "maxwell",
    "secor",
    "fluentd",
    "kubecontroller",
    "worker_plugin",
    "clicktrack",
    "services_with_ewinnnn_type_logs",
    "kiam",
    "rabbitexporter",
    "templating",
    "kafkaexporter",
    "thanos_compact_and_query",
    "deploystatus",
    "datadog_agent",
    "kafka_workers",
    "mailroom",
    "ews_timing",
    "cdm",
    "giraffe",
    "socketron",
    "vault_enterprise",
    "ruby",
    "vault_enterprise_pods",
    "vault",
    "vault_enterprise2",
    "parse_i_e_wxxx_style_logs",
    "outlook_add_in",
    "nginx_ingress_controller",
    "prometheus",
    "upgrade_ipam",
    "server_twilio",
    "exporters",
    "accounts",
    "outreach_io_azure",
    "kube_apiserver_audits",
    "voice_intelligence_orchestrator",
    "sourcegraph_namespace",
    "mysql",
    "kubernetes_cluster_autoscaler",
    "aws_alb_ingress_controller",
    "parse_key_value_logs",
    "wavefront_collector",
    "proxysql2",
    "azure",
    "azure_web",
    "azure_storage",
    "azure_network",
    "azure_compute",
    "kubecost_cost_analyzer",
    "twistlock_defender",
    "wavefront_proxy",
    "aws_node_termination_handler",
    "etcd",
    "flagship_workers__kafka___rabbit_",
    "atlantis",
    "source_oauth2_proxy",
    "glog_pipeline",
    "draino",
    "better_coredns",
    "voice_call_state_manager",
    "auth0",
    "twiliowebhook",
    "kube_scheduler__glog_",
    "aws_ecs_agent",
    "twistlock_defender2",
    "coffeeapi",
    "nodejs",
    "postgresql",
    "transcription_frontdoor",
    "auth_logs___syslog_",
    "orgservice",
    "cassandra",
    "public_calendar",
    "apache_httpd",
    "flubby",
    "voice_dialer",
    "azure_recovery_services",
    "telefork",
    "argocd_application_controller",
    "c_",
    "flagshiptriggers",
    "litmustriggers",
    "trigger_entry_point",
    "web_browser_logs",
    "voice_insights",
]

[transforms.processing.logs.pipelines.kubernetes]
name = "kubernetes"
filter.type = "datadog_search"
filter.source = ""

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.dd.service = .custom.app
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.dd.service = .custom.kube_container_name
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.dd.service = .custom.kube_deployment
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.dd.service = .custom.kube_daemon_set
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.dd.service = .custom.kube_stateful_set
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
}
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.bento = .custom.outreach.bento.name
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.bento = .custom.bento.name
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.bento = .custom.bento
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.k8s_namespace = .custom.kube_namespace
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .k8s_namespace, patterns: ["%{word}--%{word:bento}"],
aliases: {
"bootstrap_bento": "%{word}--%{word:bento}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
.custom.bento = .custom.bento
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[LaunchDarkly] %{date(\"yyyy/MM/dd HH:mm:ss\"):} %{word:ld_log_level}: Successfully initialized LaunchDarkly client!"],
aliases: {
"ld_success_level_attr": "\\[LaunchDarkly] %{date(\"yyyy/MM/dd HH:mm:ss\"):} %{word:ld_log_level}: Successfully initialized LaunchDarkly client!"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kubernetes.transforms]]
type = "remap"
source = '''
status = string(.custom.ld_log_level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.flagship]
name = "flagship"
filter.type = "datadog_search"
filter.source = "source:(server OR vidyard_integration)"

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.flagship.transforms.logs]
order = [
    "http_server_logs"
]

[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.http_server_logs]
name = "http_server_logs"
filter.type = "datadog_search"
filter.source = "service:server*"

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.http_server_logs.transforms]]
type = "remap"
source = '''
if exists(.custom.path) {
.custom.http.url_details, err = parse_url(.custom.path)
}
'''

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.http_server_logs.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.status
'''

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.http_server_logs.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.method
'''

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.http_server_logs.transforms]]
type = "remap"
source = '''
.custom.http.referer = .custom.referer
'''

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.http_server_logs.transforms]]
type = "remap"
source = '''
.custom.http.request_id = .custom.request_id
'''

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.http_server_logs.transforms]]
type = "remap"
source = '''
.custom.usr.email = .custom.user
'''


[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
.custom.logger.name = .custom.logger
'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
.custom.logger.thread_name = .custom.thread_id
'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
.custom.error.kind = .custom.error.class
'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
.custom.error.stack = .custom.error.backtrace
'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.service_time * 1
'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
.custom.or.org.shortname = .custom.org
'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.flagship.transforms.logs]
order = [
    "tenantresourcehealthconsumer"
]

[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.tenantresourcehealthconsumer]
name = "tenantresourcehealthconsumer"
filter.type = "datadog_search"
filter.source = "@async.consumer:TenantResourceHealthConsumer"

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.tenantresourcehealthconsumer.transforms]]
type = "remap"
source = '''
.custom.resource.type = .custom.resource
'''


[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
.custom.oauth.application = .custom.oauth_application
'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.http.status_category = "2xx"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.http.status_category = "3xx"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.http.status_category = "4xx"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.http.status_category = "5xx"
}

'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@total_entries:0") {
.custom.total_entries_category = "0"
} else if match_datadog_query(.,"@total_entries:[1 TO 10]") {
.custom.total_entries_category = "1-10"
} else if match_datadog_query(.,"@total_entries:[11 TO 20]") {
.custom.total_entries_category = "11-20"
} else if match_datadog_query(.,"@total_entries:[21 TO 50]") {
.custom.total_entries_category = "21-50"
} else if match_datadog_query(.,"@total_entries:>50") {
.custom.total_entries_category = "50+"
}

'''

[[transforms.processing.logs.pipelines.flagship.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.flagship.transforms.logs]
order = [
    "elasticsearch_clientproxy"
]

[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.elasticsearch_clientproxy]
name = "elasticsearch_clientproxy"
filter.type = "datadog_search"
filter.source = "@logger.name:\"Elasticsearch::ClientProxy::QueryRunner\""

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.elasticsearch_clientproxy.transforms]]
type = "remap"
source = '''
.custom.elasticsearch.client_proxy.es7_es1_elastic_time_pct,err = (.custom.elasticsearch.client_proxy.es7_elastic_time_ms - .custom.elasticsearch.client_proxy.es1_elastic_time_ms) / (.custom.elasticsearch.client_proxy.es7_elastic_time_ms + .custom.elasticsearch.client_proxy.es1_elastic_time_ms) * 2 * 100
'''

[[transforms.processing.logs.pipelines.flagship.transforms.logs.pipelines.elasticsearch_clientproxy.transforms]]
type = "remap"
source = '''
.custom.elasticsearch.client_proxy.es7_es1_processing_time_pct,err = (.custom.elasticsearch.client_proxy.es7_processing_time_ms - .custom.elasticsearch.client_proxy.es1_processing_time_ms) / (.custom.elasticsearch.client_proxy.es7_processing_time_ms + .custom.elasticsearch.client_proxy.es1_processing_time_ms) * 2 * 100
'''


[transforms.processing.logs.pipelines.nginx]
name = "nginx"
filter.type = "datadog_search"
filter.source = "source:nginx"

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)","%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*","%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, %{data})?"],
aliases: {
"access.common": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
"access.combined": "%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
"error.format": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, )?",
"_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}",
"_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.client
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .request, patterns: ["(?>%{_method} |)%{_url}(?> %{_version}|)"],
aliases: {
"request_parsing": "(?>%{_method} |)%{_url}(?> %{_version}|)",
"_method": "%{word:http.method}",
"_url": "%{notSpace:http.url}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if exists(.custom.http.url) {
.custom.http.url_details, err = parse_url(.custom.http.url)
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if exists(.custom.http.useragent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.http.useragent)
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}

'''

[[transforms.processing.logs.pipelines.nginx.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.envoy]
name = "envoy"
filter.type = "datadog_search"
filter.source = "source:envoy"

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):date}\\] \"%{word:http.method} %{notSpace:http.url} %{notSpace:http.protocol}\" %{integer:http.status_code} %{data:response.flags} %{integer:network.bytes_read} %{integer:network.bytes_written} %{integer:duration} (-|%{integer:response.x_envoy_upstream_service_time}) \"%{data:http._x_forwarded_for}\" \"%{data:http.useragent}\" \"%{notSpace:http.request_id}\" \"%{notSpace:http.authority}\" %{data:upstream_host}"],
aliases: {
"access_log": "\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):date}\\] \"%{word:http.method} %{notSpace:http.url} %{notSpace:http.protocol}\" %{integer:http.status_code} %{data:response.flags} %{integer:network.bytes_read} %{integer:network.bytes_written} %{integer:duration} (-|%{integer:response.x_envoy_upstream_service_time}) \"%{data:http._x_forwarded_for}\" \"%{data:http.useragent}\" \"%{notSpace:http.request_id}\" \"%{notSpace:http.authority}\" %{data:upstream_host}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
if exists(.custom.http.useragent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.http.useragent)
}
'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}

'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
if exists(.custom.http.url) {
.custom.http.url_details, err = parse_url(.custom.http.url)
}
'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.url_details.path:/voicemails*") {
.custom.http.ingress_endpoint = "voicemails"
} else if match_datadog_query(.,"@http.url_details.path:/voice_plugins*") {
.custom.http.ingress_endpoint = "voice_plugins"
} else if match_datadog_query(.,"@http.url_details.path:/twilio*") {
.custom.http.ingress_endpoint = "twilio"
} else if match_datadog_query(.,"@http.url_details.path:/status*") {
.custom.http.ingress_endpoint = "status"
} else if match_datadog_query(.,"@http.url_details.path:/slack*") {
.custom.http.ingress_endpoint = "slack"
} else if match_datadog_query(.,"@http.url_details.path:/recordings*") {
.custom.http.ingress_endpoint = "recordings"
} else if match_datadog_query(.,"@http.url_details.path:/private*") {
.custom.http.ingress_endpoint = "private"
} else if match_datadog_query(.,"@http.url_details.path:/office365*") {
.custom.http.ingress_endpoint = "office365"
} else if match_datadog_query(.,"@http.url_details.path:/internal*") {
.custom.http.ingress_endpoint = "internal"
} else if match_datadog_query(.,"@http.url_details.path:/hooks*") {
.custom.http.ingress_endpoint = "hooks"
} else if match_datadog_query(.,"@http.url_details.path:/google*") {
.custom.http.ingress_endpoint = "google"
} else if match_datadog_query(.,"@http.url_details.path:/ews*") {
.custom.http.ingress_endpoint = "ews"
} else if match_datadog_query(.,"@http.url_details.path:/api/v2*") {
.custom.http.ingress_endpoint = "apiv2"
} else if match_datadog_query(.,"@http.url_details.path:/bento/status*") {
.custom.http.ingress_endpoint = "bentostatus"
} else if match_datadog_query(.,"@http.url_details.path:/api/events*") {
.custom.http.ingress_endpoint = "apievents"
} else if match_datadog_query(.,"@http.url_details.path:/api* -@http.url_details.path:/api/v2* -@http.url_details.path:/api/events*") {
.custom.http.ingress_endpoint = "api"
} else if match_datadog_query(.,"@http.url_details.path:/assets*") {
.custom.http.ingress_endpoint = "assets"
} else if match_datadog_query(.,"@http.url_details.path:/graphql*") {
.custom.http.ingress_endpoint = "graphql"
} else if match_datadog_query(.,"@http.url_details.path:/360*") {
.custom.http.ingress_endpoint = "360"
} else if match_datadog_query(.,"@http.url_details.path:/reports*") {
.custom.http.ingress_endpoint = "reports"
} else if match_datadog_query(.,"@http.url_details.path:/tasks*") {
.custom.http.ingress_endpoint = "tasks"
} else if match_datadog_query(.,"@http.url_details.path:/prospects*") {
.custom.http.ingress_endpoint = "prospects"
} else if match_datadog_query(.,"@http.url_details.path:/") {
.custom.http.ingress_endpoint = "root"
} else if match_datadog_query(.,"@http.url_details.path:/templates*") {
.custom.http.ingress_endpoint = "templates"
} else if match_datadog_query(.,"@http.url_details.path:/sequences*") {
.custom.http.ingress_endpoint = "sequences"
} else if match_datadog_query(.,"@http.url_details.path:/sounds*") {
.custom.http.ingress_endpoint = "sounds"
} else if match_datadog_query(.,"@http.url_details.path:/accounts*") {
.custom.http.ingress_endpoint = "accounts"
} else if match_datadog_query(.,"@http.url_details.path:/users*") {
.custom.http.ingress_endpoint = "users"
} else if match_datadog_query(.,"@http.url_details.path:/admin*") {
.custom.http.ingress_endpoint = "admin"
} else if match_datadog_query(.,"@http.url_details.path:/outbox*") {
.custom.http.ingress_endpoint = "outbox"
} else if match_datadog_query(.,"@http.url_details.path:/calls*") {
.custom.http.ingress_endpoint = "calls"
}

'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.http._x_forwarded_for
'''

[[transforms.processing.logs.pipelines.envoy.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.http._x_forwarded_for
'''

[transforms.processing.logs.pipelines.portus]
name = "portus"
filter.type = "datadog_search"
filter.source = "source:registry"

[[transforms.processing.logs.pipelines.portus.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [""],
aliases: {
"rule": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.portus.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{notSpace:upstream_host} %{notSpace:identity} %{notSpace:user} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date}\\] \"%{word:http.method} %{notSpace:http.url_details.path} %{notSpace:http.protocol}\" %{number:http.status_code} %{number:http.size} \"%{data:http.referer}\" \"%{data:http.useragent}\".*"],
aliases: {
"access.log": "%{notSpace:upstream_host} %{notSpace:identity} %{notSpace:user} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date}\\] \"%{word:http.method} %{notSpace:http.url_details.path} %{notSpace:http.protocol}\" %{number:http.status_code} %{number:http.size} \"%{data:http.referer}\" \"%{data:http.useragent}\".*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.portus.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.http.request.uri
'''

[[transforms.processing.logs.pipelines.portus.transforms]]
type = "remap"
source = '''
.custom.upstream_host = .custom.http.request.remoteaddr
'''

[[transforms.processing.logs.pipelines.portus.transforms]]
type = "remap"
source = '''
.custom.http.useragent = .custom.http.request.useragent
'''

[transforms.processing.logs.pipelines.redis]
name = "redis"
filter.type = "datadog_search"
filter.source = "source:redis"

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_pid}:%{_role} %{_date} %{_severity} %{data:message}"],
aliases: {
"default_format": "%{_pid}:%{_role} %{_date} %{_severity} %{data:message}",
"_date": "(%{date(\"dd MMM HH:mm:ss.SSS\"):date}|%{date(\"dd MMM yyyy HH:mm:ss.SSS\"):date})",
"_pid": "%{integer:pid}",
"_severity": "%{notSpace:severity}",
"_role": "%{word:role}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
}
'''

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@severity: \".\"") {
.custom.redis.severity = "debug"
} else if match_datadog_query(.,"@severity: \"-\"") {
.custom.redis.severity = "verbose"
} else if match_datadog_query(.,"@severity: \"*\"") {
.custom.redis.severity = "notice"
} else if match_datadog_query(.,"@severity: \"#\"") {
.custom.redis.severity = "warning"
}

'''

[[transforms.processing.logs.pipelines.redis.transforms]]
type = "remap"
source = '''
status = string(.custom.redis.severity) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.consul]
name = "consul"
filter.type = "datadog_search"
filter.source = "source:consul"

[[transforms.processing.logs.pipelines.consul.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["==>\\s+(?>%{_status}:|)%{data:message}","(\\s*|\\t*)%{_date_2} %{_hostname} %{_app}\\[%{_thread_id}]:\\s+%{data:message}","(\\s*|\\t*)(%{_date}|%{_date_3}) \\[%{_status}\\] %{_app}:( %{_event}:)?%{data:message}"],
aliases: {
"Upstart_format": "==>\\s+(?>%{_status}:|)%{data:message}",
"Consul_template": "(\\s*|\\t*)%{_date_2} %{_hostname} %{_app}\\[%{_thread_id}]:\\s+%{data:message}",
"Default_format": "(\\s*|\\t*)(%{_date}|%{_date_3}) \\[%{_status}\\] %{_app}:( %{_event}:)?%{data:message}",
"_date": "%{date(\"yyyy/MM/dd HH:mm:ss\"):timestamp}",
"_date_2": "%{date(\"MMM dd HH:mm:ss\"):timestamp}",
"_date_3": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZZ\"):timestamp}",
"_status": "%{word:level}",
"_app": "%{data:app}",
"_event": "%{word:event}",
"_hostname": "%{notSpace:hostname}",
"_thread_id": "%{integer:logger.thread.id}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.consul.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.consul.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.python]
name = "python"
filter.type = "datadog_search"
filter.source = "source:python"

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(%{_python_prefix}|%{_datadog_prefix})\\s+Traceback \\(most recent call last\\):\\s*%{data:error.stack}","(%{_python_prefix}|%{_datadog_prefix})\\s+","%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+((\\n|\\t)%{data:error.stack})?"],
aliases: {
"traceback_format": "(%{_python_prefix}|%{_datadog_prefix})\\s+Traceback \\(most recent call last\\):\\s*%{data:error.stack}",
"python_format": "(%{_python_prefix}|%{_datadog_prefix})\\s+",
"python_fallback": "%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+((\\n|\\t)%{data:error.stack})?",
"_datadog_prefix": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp} %{word:levelname}\\s+\\[%{notSpace:process.name}\\]\\s+\\[%{notSpace:filename}:%{number:lineno}\\]\\s+\\[dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\] -",
"_python_prefix": "%{date(\"yyyy-MM-dd'T'HH:mm:ss','SSS\"):timestamp}\\s+%{word:levelname}\\s+\\[%{notSpace:process.name}\\]\\s+\\[%{integer:process.id}\\]"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
status = string(.custom.levelname) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
.custom.error.stack = .custom.traceback
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
.custom.logger.name = .custom.name
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
.custom.logger.thread_name = .custom.threadName
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack, patterns: ["File \"%{notSpace:filename}\", line %{integer:lineno}.*\\s+%{regex(\"[a-zA-Z]*Error[a-zA-Z]*\"):error.kind}: %{data:error.message}"],
aliases: {
"parsing_traceback": "File \"%{notSpace:filename}\", line %{integer:lineno}.*\\s+%{regex(\"[a-zA-Z]*Error[a-zA-Z]*\"):error.kind}: %{data:error.message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
.custom.env = .custom.dd.env
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
.custom.version = .custom.dd.version
'''

[[transforms.processing.logs.pipelines.python.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
}
'''

[transforms.processing.logs.pipelines.rabbitmq]
name = "rabbitmq"
filter.type = "datadog_search"
filter.source = "source:rabbitmq"

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["=%{_status} REPORT(====)? (%{_date_d}|%{_date_dd}) =(==)?\\s*%{data:message}","%{_date_latest}\\s\\[%{_status}\\]\\s*%{data:message}"],
aliases: {
"rabbit_default": "=%{_status} REPORT(====)? (%{_date_d}|%{_date_dd}) =(==)?\\s*%{data:message}",
"rabbit_latest": "%{_date_latest}\\s\\[%{_status}\\]\\s*%{data:message}",
"_date_d": "%{date(\"d-MMM-yyyy::HH:mm:ss\"):timestamp}",
"_date_dd": "%{date(\"dd-MMM-yyyy::HH:mm:ss\"):timestamp}",
"_date_latest": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):timestamp}",
"_status": "%{word:status}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.rabbitmq.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[transforms.processing.logs.pipelines.vpa_xx]
name = "vpa_xx"
filter.type = "datadog_search"
filter.source = "source:(vpa-updater OR vpa-recommender OR vpa-admission-controller)"

[[transforms.processing.logs.pipelines.vpa_xx.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"[EIW]\"):level}%{regex(\"[0-9]{2}\"):mm}%{regex(\"[0-9]{2}\"):dd} %{date(\"HH:mm:ss.SSSSSS\"):time} .*"],
aliases: {
"klog_parse": "%{regex(\"[EIW]\"):level}%{regex(\"[0-9]{2}\"):mm}%{regex(\"[0-9]{2}\"):dd} %{date(\"HH:mm:ss.SSSSSS\"):time} .*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.vpa_xx.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.zookeeper]
name = "zookeeper"
filter.type = "datadog_search"
filter.source = "source:zookeeper"

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?","%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?"],
aliases: {
"Zookeeper_default": "(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"Zookeeper_recommended": "%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"_date": "%{date(\"yyyy-MM-dd HH:mm:ss\"):timestamp}",
"_date_ms": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
"_duration": "%{integer:duration}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_status": "%{word:status}",
"_logger_name": "%{notSpace:logger.name}",
"_context": "%{notSpace:logger.context}",
"_line": "%{integer:line}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack, patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.zookeeper.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.elasticsearch]
name = "elasticsearch"
filter.type = "datadog_search"
filter.source = "source:elasticsearch"

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.search.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\]\\[%{_shard}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*","\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.indexing.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*","\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[%{_logger}\\s*\\]\\s*(\\[%{_node}\\])?.*"],
aliases: {
"Elasticsearch_search_query": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.search.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\]\\[%{_shard}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
"Elasticsearch_slow_indexing": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[index.indexing.slowlog.%{_operation}\\] (\\[%{_node}\\] )?\\[%{_index}\\] took\\[.*\\], took_millis\\[%{_duration}\\].*",
"Elasticsearch_default": "\\[(?>%{_date}|%{_date_format2})\\]\\[%{_status}\\s*\\]\\[%{_logger}\\s*\\]\\s*(\\[%{_node}\\])?.*",
"_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss,SSS\"):timestamp}",
"_date_format2": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
"_status": "%{word:level}",
"_operation": "%{notSpace:elasticsearch.operation}",
"_node": "%{hostname:nodeId}",
"_index": "%{notSpace:elasticsearch.index}",
"_shard": "%{integer:elasticsearch.shard}",
"_duration": "%{integer:duration:scale(1000000)}",
"_logger": "%{notSpace:logger.name}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.elasticsearch.transforms]]
type = "remap"
source = '''
.custom.node_name = .custom.nodeId
'''

[transforms.processing.logs.pipelines.kafka]
name = "kafka"
filter.type = "datadog_search"
filter.source = "source:kafka"

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?","%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?","\\[%{_date_ms}\\] %{_status} %{data:msg} \\(%{_logger_name}\\)"],
aliases: {
"Kafka_default": "(%{_date_ms}|%{_duration})\\s+\\[%{_thread_name}\\]\\s+%{_status}\\s+%{_logger_name}\\s*(%{_context}\\s*)?- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"Kafka_recommended": "%{_date} %{_status}\\s+%{_logger_name}:%{_line}\\s+- %{data:msg}((\\n|\\t)%{data:error.stack})?",
"Kafka_standard": "\\[%{_date_ms}\\] %{_status} %{data:msg} \\(%{_logger_name}\\)",
"_date": "%{date(\"yyyy-MM-dd HH:mm:ss\"):timestamp}",
"_date_ms": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):timestamp}",
"_duration": "%{integer:duration}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_status": "%{word:status}",
"_logger_name": "%{notSpace:logger.name}",
"_context": "%{notSpace:logger.context}",
"_line": "%{integer:line}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack, patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.kafka.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.proxysql]
name = "proxysql"
filter.type = "datadog_search"
filter.source = "source:proxysql"

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"yyyy-MM-dd HH:mm:ss\"):time} %{data:action}: \\[%{data:level}\\] %{regex(\".*\"):message}"],
aliases: {
"Rule": "%{date(\"yyyy-MM-dd HH:mm:ss\"):time} %{data:action}: \\[%{data:level}\\] %{regex(\".*\"):message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"yyyy-MM-dd HH:mm:ss\"):time} \\[%{data:level}\\] %{regex(\".*\"):message}"],
aliases: {
"Rule": "%{date(\"yyyy-MM-dd HH:mm:ss\"):time} \\[%{data:level}\\] %{regex(\".*\"):message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [".*%{regex(\"(mysql.*?intor.io)\"):proxysql.mysql_shard}.*"],
aliases: {
"ExtractShard": ".*%{regex(\"(mysql.*?intor.io)\"):proxysql.mysql_shard}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [".*outreach_production_%{regex(\"[a-zA-Z_0-9]*\"):or.org.shortname}.",".*outreach_staging_%{regex(\"[a-zA-Z_0-9]*\"):or.org.shortname}."],
aliases: {
"GetShortnameProd": ".*outreach_production_%{regex(\"[a-zA-Z_0-9]*\"):or.org.shortname}.",
"GetShortnameStaging": ".*outreach_staging_%{regex(\"[a-zA-Z_0-9]*\"):or.org.shortname}."
})
.custom, err = merge(.custom, custom)
'''

[transforms.processing.logs.pipelines.ingestitron]
name = "ingestitron"
filter.type = "datadog_search"
filter.source = "source:ingestitron"

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.org = .custom.outreach.org.shortname
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.ingestitron.transforms.logs]
order = [
    "http_request"
]

[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.http_request]
name = "http_request"
filter.type = "datadog_search"
filter.source = "@eventName:CachingHttpClient#executeRaw"

[[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.http_request.transforms]]
type = "remap"
source = '''
if exists(.custom.content.url) {
.custom.http.url_details, err = parse_url(.custom.content.url)
}
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.http_request.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.content.method
'''


[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.usr.email = .custom.outreach.user.email
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
if exists(.custom.browser.ua) {
.custom.http.useragent_details,err = parse_user_agent(.custom.browser.ua)
}
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.or.voice_trace_id = .custom.voiceTraceId
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.additionalData.statusCode
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.status = .custom.http.status_code
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.error.cause.kind = .custom.eventProperties.message
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.deployment.bento = .custom.bento
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.ingestitron.transforms.logs]
order = [
    "socketron_client"
]

[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.socketron_client]
name = "socketron_client"
filter.type = "datadog_search"
filter.source = "@logger.name:socketron-client"

[[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.socketron_client.transforms]]
type = "remap"
source = '''
.custom.or.org.guid = .custom.org.id
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.socketron_client.transforms]]
type = "remap"
source = '''
.custom.or.org.shortname = .custom.org.shortname
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.socketron_client.transforms]]
type = "remap"
source = '''
.custom.usr.email = .custom.user.email
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.socketron_client.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.user.id
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms.logs.pipelines.socketron_client.transforms]]
type = "remap"
source = '''
.custom.or.org.bento = .custom.bento.name
'''


[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@duration:[0 TO 100]") {
.custom.time_bucket = "0-100ms"
} else if match_datadog_query(.,"@duration:[100 TO 500]") {
.custom.time_bucket = "100-500ms"
} else if match_datadog_query(.,"@duration:[500 TO 1000]") {
.custom.time_bucket = "500-1000ms"
} else if match_datadog_query(.,"@duration:[1000 TO 5000]") {
.custom.time_bucket = "1000-5000ms"
} else if match_datadog_query(.,"@duration:[5000 TO 20000]") {
.custom.time_bucket = "5000-20000ms"
} else if match_datadog_query(.,"@duration:[20000 TO 100000]") {
.custom.time_bucket = "20000-100000ms"
} else if match_datadog_query(.,"@duration:>100000") {
.custom.time_bucket = ">100000ms"
} else if match_datadog_query(.,"@duration:<0") {
.custom.time_bucket = "Negative"
}

'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@additionalData.numberOfCalendars:[3 TO 5]") {
.custom.calendar_count = "3-5 calendars"
} else if match_datadog_query(.,"@additionalData.numberOfCalendars:[1 TO 2]") {
.custom.calendar_count = "1-2 calendars"
} else if match_datadog_query(.,"@additionalData.numberOfCalendars:<1") {
.custom.calendar_count = "no calendar"
} else if match_datadog_query(.,"@additionalData.numberOfCalendars:[6 TO 9]") {
.custom.calendar_count = "6-9 calendars"
} else if match_datadog_query(.,"@additionalData.numberOfCalendars:>9") {
.custom.calendar_count = "10+ calendars"
}

'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.error.message = .custom.eventProperties.error
'''

[[transforms.processing.logs.pipelines.ingestitron.transforms]]
type = "remap"
source = '''
.custom.timeSinceOrigin = .custom.resources.memory.timeSinceOrigin
'''

[transforms.processing.logs.pipelines.couchdb]
name = "couchdb"
filter.type = "datadog_search"
filter.source = "source:couchdb"

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[%{_date_access}\\] \\[%{_level}\\] %{notSpace} %{_client_ip} - - %{_method} %{_url} %{_status_code}.*","\\[%{_level}\\] %{_date} %{_user}@%{_client_ip} %{notSpace} (-------- CRASH REPORT\\s*%{data:error.stack}|%{data})","%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date_access} \\[%{_level}\\] .*"],
aliases: {
"http_rule": "\\[%{_date_access}\\] \\[%{_level}\\] %{notSpace} %{_client_ip} - - %{_method} %{_url} %{_status_code}.*",
"default_format": "\\[%{_level}\\] %{_date} %{_user}@%{_client_ip} %{notSpace} (-------- CRASH REPORT\\s*%{data:error.stack}|)",
"fallback_format": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date_access} \\[%{_level}\\] .*",
"_date_access": "(%{date(\"EEE, dd MMM yyyy HH:mm:ss z\"):date_access}|%{date(\"EEE, d MMM yyyy HH:mm:ss z\"):date_access})",
"_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"):date_access}",
"_level": "%{word:level}",
"_method": "%{word:http.method}",
"_url": "%{notSpace:http.url}",
"_status_code": "%{number:http.status_code}",
"_client_ip": "%{notSpace:network.client.ip}",
"_user": "%{notSpace:db.user}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.couchdb.transforms]]
type = "remap"
source = '''
if exists(.custom.http.url) {
.custom.http.url_details, err = parse_url(.custom.http.url)
}
'''

[transforms.processing.logs.pipelines.docker]
name = "docker"
filter.type = "datadog_search"
filter.source = "source:docker"

[[transforms.processing.logs.pipelines.docker.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_level}%{_kube_date}\\s+%{_thread_id} %{_logger_name}:%{_line}\\].*","%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*","(?>\\[ %{word:process.name} \\] )?%{_dd_date} \\| %{word:level} \\| \\(%{_logger_name}:%{_line} in %{word:datadog.process}\\) \\| .*"],
aliases: {
"Kubernetes_format": "%{_level}%{_kube_date}\\s+%{_thread_id} %{_logger_name}:%{_line}\\].*",
"access.combined": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
"datadog_format": "(?>\\[ %{word:process.name} \\] )?%{_dd_date} \\| %{word:level} \\| \\(%{_logger_name}:%{_line} in %{word:datadog.process}\\) \\| .*",
"_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):timestamp}",
"_dd_date": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp}",
"_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}",
"_level": "%{regex(\"[\\\\w]\"):level}",
"_kube_date": "%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}",
"_thread_id": "%{number:logger.thread_id}",
"_logger_name": "%{notSpace:logger.name}",
"_line": "%{number:lineno}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.docker.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.docker.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.journald]
name = "journald"
filter.type = "datadog_search"
filter.source = "source:journald -service:kubelet"

[[transforms.processing.logs.pipelines.journald.transforms]]
type = "remap"
source = '''
if exists(.custom.journald._COMM) {
.service = .custom.journald._COMM
}
'''

[[transforms.processing.logs.pipelines.journald.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [""],
aliases: {
"rule": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.journald.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.journald.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.calico_parser]
name = "calico_parser"
filter.type = "datadog_search"
filter.source = "service:(calico-node OR calico-kube-controllers)"

[[transforms.processing.logs.pipelines.calico_parser.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):timestamp} \\[%{word:level}\\]\\[%{number:thread_id}\\] %{regex(\"[a-z.]*\"):logger_name} %{number:line_number}:\\s+%{data:message}","%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):timestamp} \\[%{word:level}\\]\\[%{number:thread_id}\\] %{regex(\"[a-z_.]*\"):logger_name} %{number:line_number}:\\s+%{data:message}"],
aliases: {
"calico": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):timestamp} \\[%{word:level}\\]\\[%{number:thread_id}\\] %{regex(\"[a-z.]*\"):logger_name} %{number:line_number}:\\s+%{data:message}",
"calicokube": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):timestamp} \\[%{word:level}\\]\\[%{number:thread_id}\\] %{regex(\"[a-z_.]*\"):logger_name} %{number:line_number}:\\s+%{data:message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.calico_parser.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.requeue]
name = "requeue"
filter.type = "datadog_search"
filter.source = "source:requeue"

[[transforms.processing.logs.pipelines.requeue.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"HH:mm:ss.SSS\"):time} \\[%{word:level}.*"],
aliases: {
"requeue_rule": "%{date(\"HH:mm:ss.SSS\"):time} \\[%{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.requeue.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.maxwell]
name = "maxwell"
filter.type = "datadog_search"
filter.source = "source:maxwell"

[[transforms.processing.logs.pipelines.maxwell.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"HH:mm:ss,SSS\"):time} %{word:level}.*"],
aliases: {
"mxw_rule": "%{date(\"HH:mm:ss,SSS\"):time} %{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.maxwell.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.secor]
name = "secor"
filter.type = "datadog_search"
filter.source = "source:secor"

[[transforms.processing.logs.pipelines.secor.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"YYYY-MM-DD HH:mm:ss,SSS\"):date} \\[%{notSpace}\\] \\(%{notSpace}\\) %{word:level}.*"],
aliases: {
"secor_rule": "%{date(\"YYYY-MM-DD HH:mm:ss,SSS\"):date} \\[%{notSpace}\\] \\(%{notSpace}\\) %{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.secor.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.fluentd]
name = "fluentd"
filter.type = "datadog_search"
filter.source = "source:fluentd"

[[transforms.processing.logs.pipelines.fluentd.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"YYY-MM-DD HH:mm:ss +SSSS\"):time} \\[%{word:level}\\].*"],
aliases: {
"fluentd": "%{date(\"YYY-MM-DD HH:mm:ss +SSSS\"):time} \\[%{word:level}\\].*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.fluentd.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.kubecontroller]
name = "kubecontroller"
filter.type = "datadog_search"
filter.source = "source:(kube-controller-manager OR kube-state-metrics)"

[[transforms.processing.logs.pipelines.kubecontroller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\".\"):level}.*Event.*Kind:\"%{word:event_resource}\".*Namespace:\"%{data:kube_namespace}\", Name:\"%{data:pod_name}\".*reason: '%{data:event_type}'.*New size: %{integer:pod_count}.*","%{regex(\".\"):level}.*Event.*Kind:\"%{word:event_resource}\".*Namespace:\"%{data:kube_namespace}\", Name:\"%{data:pod_name}\".*reason: '%{data:event_type}'.*","%{regex(\".\"):level}.*"],
aliases: {
"event_rescale_parsing_rule": "%{regex(\".\"):level}.*Event.*Kind:\"%{word:event_resource}\".*Namespace:\"%{data:kube_namespace}\", Name:\"%{data:pod_name}\".*reason: '%{data:event_type}'.*New size: %{integer:pod_count}.*",
"generic_event_parsing_rule": "%{regex(\".\"):level}.*Event.*Kind:\"%{word:event_resource}\".*Namespace:\"%{data:kube_namespace}\", Name:\"%{data:pod_name}\".*reason: '%{data:event_type}'.*",
"level_rule": "%{regex(\".\"):level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kubecontroller.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"service:kube-controller-manager AND @level:I*") {
.custom.level = "info"
} else if match_datadog_query(.,"service:kube-controller-manager AND @level:E*") {
.custom.level = "error"
} else if match_datadog_query(.,"service:kube-controller-manager AND @level:W*") {
.custom.level = "warn"
}

'''

[[transforms.processing.logs.pipelines.kubecontroller.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.worker_plugin]
name = "worker_plugin"
filter.type = "datadog_search"
filter.source = "service:worker-plugin"

[[transforms.processing.logs.pipelines.worker_plugin.transforms]]
type = "remap"
source = '''
.custom.logger.name = .custom.logger
'''

[[transforms.processing.logs.pipelines.worker_plugin.transforms]]
type = "remap"
source = '''
.custom.error.kind = .custom.error.class
'''

[transforms.processing.logs.pipelines.clicktrack]
name = "clicktrack"
filter.type = "datadog_search"
filter.source = "service:clicktrack"

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.verb
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.status
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.http.request_id = .custom.request_id
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.path
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.tags.status
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.or.org.bento = .custom.bento
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.service = .custom.tags.service
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.timing.service_time = .custom.values.controller_action_time
'''

[[transforms.processing.logs.pipelines.clicktrack.transforms]]
type = "remap"
source = '''
.custom.duration = .custom.values.controller_action_time
'''

[transforms.processing.logs.pipelines.services_with_ewinnnn_type_logs]
name = "services_with_ewinnnn_type_logs"
filter.type = "datadog_search"
filter.source = "service:(custom-metrics-apiserver OR metrics-server-amd64 OR limiter OR efs-provisioner)"

[[transforms.processing.logs.pipelines.services_with_ewinnnn_type_logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{word:level}.*"],
aliases: {
"ms": "%{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.services_with_ewinnnn_type_logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.kiam]
name = "kiam"
filter.type = "datadog_search"
filter.source = "service:kiam"

[[transforms.processing.logs.pipelines.kiam.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{word:level}.*"],
aliases: {
"kiam": "%{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kiam.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.rabbitexporter]
name = "rabbitexporter"
filter.type = "datadog_search"
filter.source = "service:rabbitmq-exporter"

[[transforms.processing.logs.pipelines.rabbitexporter.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [""],
aliases: {
"rule": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.rabbitexporter.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.templating]
name = "templating"
filter.type = "datadog_search"
filter.source = "source:templating"

[[transforms.processing.logs.pipelines.templating.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.res.statusCode
'''

[[transforms.processing.logs.pipelines.templating.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.req.method
'''

[[transforms.processing.logs.pipelines.templating.transforms]]
type = "remap"
source = '''
.custom.duration = .custom.responseTime
'''

[[transforms.processing.logs.pipelines.templating.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.req.url
'''

[transforms.processing.logs.pipelines.kafkaexporter]
name = "kafkaexporter"
filter.type = "datadog_search"
filter.source = "source:kafka-exporter"

[[transforms.processing.logs.pipelines.kafkaexporter.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [""],
aliases: {
"ke": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kafkaexporter.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.thanos_compact_and_query]
name = "thanos_compact_and_query"
filter.type = "datadog_search"
filter.source = "service:(thanos-compact OR thanos-query)"

[[transforms.processing.logs.pipelines.thanos_compact_and_query.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [""],
aliases: {
"tc": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.thanos_compact_and_query.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.deploystatus]
name = "deploystatus"
filter.type = "datadog_search"
filter.source = "service:(deploy-status OR deploy-channel-status)"

[[transforms.processing.logs.pipelines.deploystatus.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"yyyy/MM/dd HH:mm:ss.SSSSSS\"):date} \\[%{word:level}.*"],
aliases: {
"ds": "%{date(\"yyyy/MM/dd HH:mm:ss.SSSSSS\"):date} \\[%{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.deploystatus.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.datadog_agent]
name = "datadog_agent"
filter.type = "datadog_search"
filter.source = "source:(agent OR datadog-agent OR datadog-agent-cluster-worker OR datadog-cluster-agent OR process-agent OR security-agent OR system-probe OR trace-agent)"

[[transforms.processing.logs.pipelines.datadog_agent.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["        %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\) \\|(  \\|)?( - \\|)?( \\(%{notSpace:pyFilename}:%{number:pyLineno}\\) \\|)?%{data}","%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\)%{data}","     %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level}\\s+\\| %{word:class} \\| %{data}"],
aliases: {
"agent_rule": "        %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\) \\|(  \\|)?( - \\|)?( \\(%{notSpace:pyFilename}:%{number:pyLineno}\\) \\|)?%{data}",
"agent_rule_pre_611": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{word:level} \\| \\(%{notSpace:filename}:%{number:lineno} in %{word:process}\\)%{data}",
"jmxfetch_rule": "     %{date(\"yyyy-MM-dd HH:mm:ss z\"):timestamp} \\| %{notSpace:agent} \\| %{word:level}\\s+\\| %{word:class} \\| %{data}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.datadog_agent.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.datadog_agent.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.kafka_workers]
name = "kafka_workers"
filter.type = "datadog_search"
filter.source = "service:worker-kafka*"

[[transforms.processing.logs.pipelines.kafka_workers.transforms]]
type = "remap"
source = '''
.custom.or.org.guid = .custom.org_guid
'''

[transforms.processing.logs.pipelines.mailroom]
name = "mailroom"
filter.type = "datadog_search"
filter.source = "source:mailroom"

[[transforms.processing.logs.pipelines.mailroom.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.method
'''

[[transforms.processing.logs.pipelines.mailroom.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.path
'''

[[transforms.processing.logs.pipelines.mailroom.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.status
'''

[[transforms.processing.logs.pipelines.mailroom.transforms]]
type = "remap"
source = '''
.custom.timing.service_time = .custom.duration
'''

[[transforms.processing.logs.pipelines.mailroom.transforms]]
type = "remap"
source = '''
.custom.error.cause.kind = .custom.error.kind
'''

[transforms.processing.logs.pipelines.ews_timing]
name = "ews_timing"
filter.type = "datadog_search"
filter.source = "\"<ews sync forward>\""

[[transforms.processing.logs.pipelines.ews_timing.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["Performing action <ews sync forward> took %{integer:time_ms} ms"],
aliases: {
"Rule": "Performing action <ews sync forward> took %{integer:time_ms} ms"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.ews_timing.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["Attempting action <ews sync forward> took %{integer:time_ms} ms"],
aliases: {
"Rule": "Attempting action <ews sync forward> took %{integer:time_ms} ms"
})
.custom, err = merge(.custom, custom)
'''

[transforms.processing.logs.pipelines.cdm]
name = "cdm"
filter.type = "datadog_search"
filter.source = "service:cdm-service"

[[transforms.processing.logs.pipelines.cdm.transforms]]
type = "remap"
source = '''
.custom.or.org.bento = .custom.bento
'''

[[transforms.processing.logs.pipelines.cdm.transforms]]
type = "remap"
source = '''
.custom.timing.service_time = .custom.values.controller_time
'''

[[transforms.processing.logs.pipelines.cdm.transforms]]
type = "remap"
source = '''
.custom.duration = .custom.values.controller_time
'''

[transforms.processing.logs.pipelines.giraffe]
name = "giraffe"
filter.type = "datadog_search"
filter.source = "source:giraffe"

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.or.org.bento = .custom.bento
'''

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.method
'''

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.status
'''

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.meta.req.method
'''

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.meta.req.headers.url
'''

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.meta.res.statusCode
'''

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.timing.service_time,err = .custom.meta.responseTime / 1000
'''

[[transforms.processing.logs.pipelines.giraffe.transforms]]
type = "remap"
source = '''
.custom.http.request_id = .custom.meta.requestId
'''

[transforms.processing.logs.pipelines.socketron]
name = "socketron"
filter.type = "datadog_search"
filter.source = "source:socketron"

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.or.org.bento = .custom.bento
'''

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.status
'''

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.method
'''

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.path
'''

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.http.request_id = .custom.request_id
'''

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.rails.controller = .custom.controller
'''

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.rails.controller_action = .custom.action
'''

[[transforms.processing.logs.pipelines.socketron.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.user_guid
'''

[transforms.processing.logs.pipelines.vault_enterprise]
name = "vault_enterprise"
filter.type = "datadog_search"
filter.source = "source:vault-enterprise"

[[transforms.processing.logs.pipelines.vault_enterprise.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.request.path
'''

[[transforms.processing.logs.pipelines.vault_enterprise.transforms]]
type = "remap"
source = '''
status = string(.custom.@@level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.ruby]
name = "ruby"
filter.type = "datadog_search"
filter.source = "source:ruby"

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.logger.name = .custom.logger
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.logger.thread_name = .custom.thread
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.error.stack = .custom.exception
.custom.error.stack = .custom.stack_trace
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{integer:http.status_code}(?: (?:%{word}|Internal Server Error))? in %{number:duration}ms \\((?>Views: %{number:views}ms \\| )?ActiveRecord: %{number:activerecord}ms\\)","(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} by %{notSpace:processor}.*","(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{word:http.method} \"%{notSpace:http.url_details.path}\" for %{ipOrHost:network.client.ip}.*","%{_datadog_prefix} (?>(\\n|\\t)%{data:error.stack})?","(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?Received: %{data::json}","%{_ruby_log_prefix}\\s+%{data:error.message}(?>(\\n|\\t)%{data:error.stack})?",""],
aliases: {
"completed_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{integer:http.status_code}(?: (?:%{word}|Internal Server Error))? in %{number:duration}ms \\((?>Views: %{number:views}ms \\| )?ActiveRecord: %{number:activerecord}ms\\)",
"processing_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} by %{notSpace:processor}.*",
"started_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?%{_activate_status} %{word:http.method} \"%{notSpace:http.url_details.path}\" for %{ipOrHost:network.client.ip}.*",
"datadog_format": "%{_datadog_prefix} (?>(\\n|\\t)%{data:error.stack})?",
"received_rule": "(?>%{_ruby_log_prefix}\\s+|%{_trace_rule}\\s+)?Received: %{data::json}",
"Ruby_default": "%{_ruby_log_prefix}\\s+%{data:error.message}(?>(\\n|\\t)%{data:error.stack})?",
"Ruby_keyvalue": "",
"_date": "(?:%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSS\"):date}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):date})",
"_status": "%{word:level}",
"_thread_id": "%{word:logger.thread_id}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_logger_name": "%{notSpace:logger.name}",
"_ruby_log_prefix": "%{word}, \\[%{_date} #%{_thread_id}\\]\\s+%{_status}\\s+--\\s+(?:%{_logger_name})?:(?:\\s+%{_trace_rule})?",
"_trace_rule": "\\[dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]",
"_datadog_prefix": "\\[%{date(\"yyyy-MM-dd HH:mm:ss Z\"):date}\\]\\[%{word:application}\\]\\[%{_status}\\]%{_trace_rule}",
"_activate_status": "%{word:active_directory.process_status}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack, patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.method
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.path
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.status
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.env = .custom.dd.env
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.version = .custom.dd.version
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
}
'''

[[transforms.processing.logs.pipelines.ruby.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.duration*1000000
'''

[transforms.processing.logs.pipelines.vault_enterprise_pods]
name = "vault_enterprise_pods"
filter.type = "datadog_search"
filter.source = "service:(vault-a OR vault-b)"

[[transforms.processing.logs.pipelines.vault_enterprise_pods.transforms]]
type = "remap"
source = '''
status = string(.custom.@level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.vault_enterprise_pods.transforms]]
type = "remap"
source = '''
.custom.level = .custom.@level
'''

[transforms.processing.logs.pipelines.vault]
name = "vault"
filter.type = "datadog_search"
filter.source = "source:vault"

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}","%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}"],
aliases: {
"vault_server_svc": "%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}",
"vault_server": "%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}",
"_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp}",
"_level": "%{word:level}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.auth.display_name
.custom.usr.id = .custom.auth.metatdata.username
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.request.path
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.request.data.http_status_code
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.request.remote_address
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
#TODO: geo-ip-parser
'''

[[transforms.processing.logs.pipelines.vault.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.request.operation
'''

[transforms.processing.logs.pipelines.vault_enterprise2]
name = "vault_enterprise2"
filter.type = "datadog_search"
filter.source = "source:vault-enterprise"

[[transforms.processing.logs.pipelines.vault_enterprise2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}","%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}"],
aliases: {
"vault_server_svc": "%{_date}\\s+\\[%{_level}\\]\\s+%{notSpace:vault.service}:\\s+%{data:message}",
"vault_server": "%{_date}\\s+\\[%{_level}\\]\\s+%{data:message}",
"_date": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp}",
"_level": "%{word:level}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.vault_enterprise2.transforms]]
type = "remap"
source = '''
if exists(.custom.@timestamp) {
.timestamp = .custom.@timestamp
}
'''

[[transforms.processing.logs.pipelines.vault_enterprise2.transforms]]
type = "remap"
source = '''
status = string(.custom.@level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.vault_enterprise2.transforms]]
type = "remap"
source = '''
if exists(.custom.@message) {
.message = .custom.@message
}
'''

[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs]
name = "parse_i_e_wxxx_style_logs"
filter.type = "datadog_search"
filter.source = "service:(kube-apiserver OR metrics-server OR kube-proxy OR kubelet OR kube-scheduler OR node-problem-detector OR cluster-autoscaler OR k8s-ec2-srcdst OR cert-manager OR azure-ip-masq-agent OR etcd-manager-events OR etcd-manager-main)"

[[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:thread_id} %{word:app.name}.go:%{number:lineno}] %{data:content}","%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}"],
aliases: {
"RULE_TRACE": "%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}",
"RULE_TRACE_DOTGO": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}",
"RULE_NON_TRACES": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:thread_id} %{word:app.name}.go:%{number:lineno}] %{data:content}",
"RULE_LEAST_SPECIFIC": "%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"level:E") {
.custom.level = "ERR"
} else if match_datadog_query(.,"level:I*") {
.custom.level = "INFO"
}

'''

[[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs.transforms.logs]
order = [
    "cert_manager_only"
]

[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs.transforms.logs.pipelines.cert_manager_only]
name = "cert_manager_only"
filter.type = "datadog_search"
filter.source = "service:cert-manager"

[[transforms.processing.logs.pipelines.parse_i_e_wxxx_style_logs.transforms.logs.pipelines.cert_manager_only.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{data:certmgr_action} "],
aliases: {
"RULE_CERTMANAGER": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{data:certmgr_action} "
})
.custom, err = merge(.custom, custom)
'''


[transforms.processing.logs.pipelines.outlook_add_in]
name = "outlook_add_in"
filter.type = "datadog_search"
filter.source = "@product.name:outlook"

[[transforms.processing.logs.pipelines.outlook_add_in.transforms]]
type = "remap"
source = '''
.custom.mailbox_email = .custom.outlook.mailbox.email
'''

[[transforms.processing.logs.pipelines.outlook_add_in.transforms]]
type = "remap"
source = '''
.custom.application_version = .custom.product.version
'''

[transforms.processing.logs.pipelines.nginx_ingress_controller]
name = "nginx_ingress_controller"
filter.type = "datadog_search"
filter.source = "source:nginx-ingress-controller"

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_client_ip}(?: - \\[%{notSpace}\\])? - %{_ident} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\" %{_request_size} %{_duration} \\[%{_proxy_name}\\](?: \\[%{_alternate_proxy_name}?\\])? (?:%{_upstream_ip}:%{_upstream_port}|-)(?:, %{notSpace})?(?:, %{notSpace})? (?:%{_bytes_read}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_time}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_status}|-)(?:, %{number}|, -)?(?:, %{number}|, -)?(?: %{_request_id})?.*","%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, )?","%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):date_access}\\s+%{number} %{notSpace:logger.name}:%{number:lineno}\\] .*"],
aliases: {
"access.common": "%{_client_ip}(?: - \\[%{notSpace}\\])? - %{_ident} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-) \"%{_referer}\" \"%{_user_agent}\" %{_request_size} %{_duration} \\[%{_proxy_name}\\](?: \\[%{_alternate_proxy_name}?\\])? (?:%{_upstream_ip}:%{_upstream_port}|-)(?:, %{notSpace})?(?:, %{notSpace})? (?:%{_bytes_read}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_time}|-)(?:, %{number}|, -)?(?:, %{number}|, -)? (?:%{_upstream_status}|-)(?:, %{number}|, -)?(?:, %{number}|, -)?(?: %{_request_id})?.*",
"error.format": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, )?",
"controller_format": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):date_access}\\s+%{number} %{notSpace:logger.name}:%{number:lineno}\\] .*",
"_request_id": "%{notSpace:http.request_id}",
"_upstream_status": "%{number:http.upstream_status_code}",
"_upstream_time": "%{number:http.upstream_duration}",
"_bytes_read": "%{number:network.bytes_read}",
"_upstream_port": "%{number:network.destination.port}",
"_upstream_ip": "%{ipOrHost:network.destination.ip}",
"_proxy_name": "%{notSpace:proxy.name}",
"_alternate_proxy_name": "%{notSpace:proxy.alternate_name}",
"_duration": "%{number:duration:scale(1000000000)}",
"_request_size": "%{number:network.request_size}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}",
"_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.client
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .request, patterns: ["(?>%{_method} |)%{_url}(?> %{_version}|)"],
aliases: {
"request_parsing": "(?>%{_method} |)%{_url}(?> %{_version}|)",
"_method": "%{word:http.method}",
"_url": "%{notSpace:http.url}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.http.url) {
.custom.http.url_details, err = parse_url(.custom.http.url)
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.http.useragent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.http.useragent)
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}

'''

[[transforms.processing.logs.pipelines.nginx_ingress_controller.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.prometheus]
name = "prometheus"
filter.type = "datadog_search"
filter.source = "service:prometheus"

[[transforms.processing.logs.pipelines.prometheus.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.prometheus.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[[transforms.processing.logs.pipelines.prometheus.transforms]]
type = "remap"
source = '''
if exists(.custom.ts) {
.timestamp = .custom.ts
}
'''

[[transforms.processing.logs.pipelines.prometheus.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.prometheus.transforms.logs]
order = [
    "prom_s_vault_enterprise"
]

[transforms.processing.logs.pipelines.prometheus.transforms.logs.pipelines.prom_s_vault_enterprise]
name = "prom_s_vault_enterprise"
filter.type = "datadog_search"
filter.source = "source:vault-enterprise"

[[transforms.processing.logs.pipelines.prometheus.transforms.logs.pipelines.prom_s_vault_enterprise.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZZ\"):date} \\[%{word:level}]%{space}%{regex(\"[^:][^:]*\"):component}:%{space}%{data:stuff}"],
aliases: {
"RULE_0": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZZ\"):date} \\[%{word:level}]%{space}%{regex(\"[^:][^:]*\"):component}:%{space}%{data:stuff}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.prometheus.transforms.logs.pipelines.prom_s_vault_enterprise.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''


[transforms.processing.logs.pipelines.upgrade_ipam]
name = "upgrade_ipam"
filter.type = "datadog_search"
filter.source = "service:upgrade-ipam"

[[transforms.processing.logs.pipelines.upgrade_ipam.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"YYYY-MM-DD HH:mm:ss.SSS\"):time} \\[%{word:level}.*"],
aliases: {
"ipam_rule": "%{date(\"YYYY-MM-DD HH:mm:ss.SSS\"):time} \\[%{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.upgrade_ipam.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.server_twilio]
name = "server_twilio"
filter.type = "datadog_search"
filter.source = "service:server-twilio"

[[transforms.processing.logs.pipelines.server_twilio.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[%{date(\"YYYY-MM-DD HH:mm:ss\"):time}\\] %{word:level}.*"],
aliases: {
"twilio_rule": "\\[%{date(\"YYYY-MM-DD HH:mm:ss\"):time}\\] %{word:level}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.server_twilio.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.server_twilio.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.twilio_status_code
'''

[[transforms.processing.logs.pipelines.server_twilio.transforms]]
type = "remap"
source = '''
.custom.duration = .custom.total_time
'''

[transforms.processing.logs.pipelines.exporters]
name = "exporters"
filter.type = "datadog_search"
filter.source = "service:(kafka-exporter OR mysqld-exporter OR exporter)"

[[transforms.processing.logs.pipelines.exporters.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [""],
aliases: {
"exporter": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.exporters.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.accounts]
name = "accounts"
filter.type = "datadog_search"
filter.source = "source:outreach-accounts"

[[transforms.processing.logs.pipelines.accounts.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss\"):date}\\]\\s+%{integer:logger.thread_name}\\s+%{word:level}\\s+%{regex(\"[a-zA-Z:]*\"):logger.name}\\s+%{data}"],
aliases: {
"autoFilledRule1": "\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss\"):date}\\]\\s+%{integer:logger.thread_name}\\s+%{word:level}\\s+%{regex(\"[a-zA-Z:]*\"):logger.name}\\s+%{data}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.accounts.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["<%{regex(\"[a-zA-Z:]*\"):error.class}>\\s+%{data:error.message}"],
aliases: {
"errorRule": "<%{regex(\"[a-zA-Z:]*\"):error.class}>\\s+%{data:error.message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.accounts.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@error.class:*") {
.custom.level = "ERROR"
}

'''

[[transforms.processing.logs.pipelines.accounts.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.accounts.transforms]]
type = "remap"
source = '''
.custom.logger.name = .custom.logger
'''

[[transforms.processing.logs.pipelines.accounts.transforms]]
type = "remap"
source = '''
.custom.usr.email = .custom.user
'''

[transforms.processing.logs.pipelines.outreach_io_azure]
name = "outreach_io_azure"
filter.type = "datadog_search"
filter.source = "sourcecategory:azure"

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs]
order = [
    "modify_category_info"
]

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.modify_category_info]
name = "modify_category_info"
filter.type = "datadog_search"
filter.source = ""

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.modify_category_info.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@category:(kube-audit)") {
.custom.category = "kube-apiserver-audit"
}

'''


[[transforms.processing.logs.pipelines.outreach_io_azure.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs]
order = [
    "service_remapper"
]

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.service_remapper]
name = "service_remapper"
filter.type = "datadog_search"
filter.source = ""

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.service_remapper.transforms]]
type = "remap"
source = '''
if exists(.custom.evt.category) {
.service = .custom.evt.category
} else if exists(.custom.category) {
.service = .custom.category
}
'''


[[transforms.processing.logs.pipelines.outreach_io_azure.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs]
order = [
    "environment_from_subscriptionid"
]

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.environment_from_subscriptionid]
name = "environment_from_subscriptionid"
filter.type = "datadog_search"
filter.source = "sourcecategory:azure"

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.environment_from_subscriptionid.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"subscription_id:104d9dba-bac4-4871-ae55-26ef649904cd") {
.custom.env = "ppe"
} else if match_datadog_query(.,"subscription_id:953609e8-c55e-4457-8772-032de3a333b0") {
.custom.env = "staging"
} else if match_datadog_query(.,"subscription_id:efc5a26a-40ef-4248-b111-1f21ff19722d") {
.custom.env = "production"
}

'''

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.environment_from_subscriptionid.transforms]]
type = "remap"
source = '''
.custom.env = .custom.env
'''


[[transforms.processing.logs.pipelines.outreach_io_azure.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs]
order = [
    "escape_json_log_metadata"
]

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.escape_json_log_metadata]
name = "escape_json_log_metadata"
filter.type = "datadog_search"
filter.source = ""

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.escape_json_log_metadata.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .properties.log, patterns: ["%{data::json}"],
aliases: {
"parsing_rule": "%{data::json}"
})
.custom, err = merge(.custom, custom)
'''


[[transforms.processing.logs.pipelines.outreach_io_azure.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs]
order = [
    "regions"
]

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.regions]
name = "regions"
filter.type = "datadog_search"
filter.source = ""

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.regions.transforms]]
type = "remap"
source = '''
.custom.availability_zone = .custom.azureregion
'''


[[transforms.processing.logs.pipelines.outreach_io_azure.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs]
order = [
    "cloudname"
]

[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.cloudname]
name = "cloudname"
filter.type = "datadog_search"
filter.source = ""

[[transforms.processing.logs.pipelines.outreach_io_azure.transforms.logs.pipelines.cloudname.transforms]]
type = "remap"
source = '''
.custom.Cloud = .custom.Cloud
'''


[transforms.processing.logs.pipelines.kube_apiserver_audits]
name = "kube_apiserver_audits"
filter.type = "datadog_search"
filter.source = "service:kube-apiserver-audit"

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.responseStatus.code
'''

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.requestURI
'''

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.log_level = "info"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.log_level = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.log_level = "warn"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.log_level = "error"
}

'''

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
status = string(.custom.log_level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
.custom.kube_namespace = .custom.objectRef.namespace
'''

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.verb
'''

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
.custom.kubernetes_objecttype = .custom.objectRef.resource
'''

[[transforms.processing.logs.pipelines.kube_apiserver_audits.transforms]]
type = "remap"
source = '''
.custom.kube_operation = .custom.objectRef.subresource
'''

[transforms.processing.logs.pipelines.voice_intelligence_orchestrator]
name = "voice_intelligence_orchestrator"
filter.type = "datadog_search"
filter.source = "service:orchestrator"

[[transforms.processing.logs.pipelines.voice_intelligence_orchestrator.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[mp4 @ %{regex(\"[0-9-A-Fa-f]+\"):zoom_bot.mp4}\\]\\s+%{regex(\"Application provided duration\"):zoom_bot.event}:\\s+%{data:zoom_bot.data}"],
aliases: {
"zoom_bot_mp4": "\\[mp4 @ %{regex(\"[0-9-A-Fa-f]+\"):zoom_bot.mp4}\\]\\s+%{regex(\"Application provided duration\"):zoom_bot.event}:\\s+%{data:zoom_bot.data}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.voice_intelligence_orchestrator.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@zoom_bot.event:\"Application provided duration\"") {
.custom.level = "DEBUG"
}

'''

[[transforms.processing.logs.pipelines.voice_intelligence_orchestrator.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.sourcegraph_namespace]
name = "sourcegraph_namespace"
filter.type = "datadog_search"
filter.source = "kube_namespace:sourcegraph"

[[transforms.processing.logs.pipelines.sourcegraph_namespace.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["","",""],
aliases: {
"autoFilledRule1": "",
"autoFilledRule2": "",
"autoFilledRule3": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.sourcegraph_namespace.transforms]]
type = "remap"
source = '''
status = string(.custom.lvl) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.sourcegraph_namespace.transforms]]
type = "remap"
source = '''
if exists(.custom.userAgent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.userAgent)
}
'''

[[transforms.processing.logs.pipelines.sourcegraph_namespace.transforms]]
type = "remap"
source = '''
.custom.http.method = .custom.method
'''

[[transforms.processing.logs.pipelines.sourcegraph_namespace.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[[transforms.processing.logs.pipelines.sourcegraph_namespace.transforms]]
type = "remap"
source = '''
.custom.http.status_code = .custom.code
'''

[[transforms.processing.logs.pipelines.sourcegraph_namespace.transforms]]
type = "remap"
source = '''
.custom.http.url_details.path = .custom.route
'''

[transforms.processing.logs.pipelines.mysql]
name = "mysql"
filter.type = "datadog_search"
filter.source = "source:mysql"

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(%{_timestamp}\\s+|(?:%{_rawtimestamp}\\s+)?)%{integer:thread_id}\\s+%{_operation}\\s+%{_raw_query}","(%{_timestamp}|%{_timestamp_mariadb_post_1015}) %{integer:thread_id} \\[%{_severity}\\] %{data:message}","%{_rawtimestamp}\\s+(?>InnoDB:|\\[%{_severity}\\])\\s+%{data:message}","(%{_time_line}\\s)?(%{_user_line}\\s)?\\# (%{_instance_line}\\s)?%{_set_line}\\s%{_query_line}"],
aliases: {
"query_format": "(%{_timestamp}\\s+|(?:%{_rawtimestamp}\\s+)?)%{integer:thread_id}\\s+%{_operation}\\s+%{_raw_query}",
"default_format": "(%{_timestamp}|%{_timestamp_mariadb_post_1015}) %{integer:thread_id} \\[%{_severity}\\] %{data:message}",
"raw_default_format": "%{_rawtimestamp}\\s+(?>InnoDB:|\\[%{_severity}\\])\\s+%{data:message}",
"slow_query_format": "(%{_time_line}\\s)?(%{_user_line}\\s)?\\# (%{_instance_line}\\s)?%{_set_line}\\s%{_query_line}",
"_timestamp": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ\"):db.date}",
"_timestamp_mariadb_post_1015": "(%{date(\"yyyy-MM-dd HH:mm:ss\"):db.date}|%{date(\"yyyy-MM-dd  H:mm:ss\"):db.date})",
"_rawtimestamp": "%{date(\"yyMMdd HH:mm:ss\"):db.date}",
"_severity": "%{notSpace:db.severity}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_client_port": "%{integer:network.client.port}",
"_operation": "%{word:db.operation}",
"_database": "%{word:db.instance}",
"_raw_query": "%{data:db.statement}",
"_time_line": "\\# Time: %{notSpace}(\\s+%{notSpace})?",
"_user_line": "\\# User@Host\\: %{notSpace:db.user}\\s+@\\s+%{notSpace:db.host}\\s+\\[(%{notSpace:network.client.ip})?\\](\\s+Id\\:\\s+%{number:mysql.query.id})?",
"_instance_line": "use %{notSpace:db.instance};",
"_set_line": "SET timestamp=%{number:mysql.query.timestamp};",
"_query_line": "%{data:db.slow_statement}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .db.slow_statement, patterns: ["%{word:db.operation} .*"],
aliases: {
"slow_query_format": "%{word:db.operation} .*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
.custom.db.statement = .custom.db.slow_statement
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.Query_time * 1000000000
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
.custom.db.date,err = .custom.mysql.query.timestamp * 1000
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
.custom.network.bytes_written = .custom.Bytes_sent
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
.custom.network.bytes_read = .custom.Bytes_received
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
.timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.pipelines.mysql.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler]
name = "kubernetes_cluster_autoscaler"
filter.type = "datadog_search"
filter.source = "source:cluster-autoscaler"

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"cluster_scheduler": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.kubernetes_cluster_autoscaler.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.aws_alb_ingress_controller]
name = "aws_alb_ingress_controller"
filter.type = "datadog_search"
filter.source = "source:aws-alb-ingress-controller"

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"aws_alb_ingress_controller": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.aws_alb_ingress_controller.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.parse_key_value_logs]
name = "parse_key_value_logs"
filter.type = "datadog_search"
filter.source = "service:(external-dns OR prometheus-operator OR global-trickster)"

[[transforms.processing.logs.pipelines.parse_key_value_logs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: [""],
aliases: {
"rule": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.parse_key_value_logs.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.wavefront_collector]
name = "wavefront_collector"
filter.type = "datadog_search"
filter.source = "service:wavefront-collector"

[[transforms.processing.logs.pipelines.wavefront_collector.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{data:content}","%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}",""],
aliases: {
"RULE_TRACE": "%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}",
"RULE_TRACE_DOTGO": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}",
"RULE_NON_TRACES": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{data:content}",
"RULE_LEAST_SPECIFIC": "%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}",
"wavefront": ""
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.wavefront_collector.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.proxysql2]
name = "proxysql2"
filter.type = "datadog_search"
filter.source = "source:proxysql"

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_longDate}\\s+%{_logger}\\:\\s+%{_level}\\s+%{data:message}","%{_longDate}\\s+%{_level}\\s+%{data:message}","%{_longDate}.*%{_level}%{data:message}","%{data:proxysql_service}\\s+%{_revision}\\s+--\\s+%{notSpace:logger.name}\\s+--\\s+%{_textDate}"],
aliases: {
"extendedFormat": "%{_longDate}\\s+%{_logger}\\:\\s+%{_level}\\s+%{data:message}",
"simplifiedFormat": "%{_longDate}\\s+%{_level}\\s+%{data:message}",
"safeGuard": "%{_longDate}.*%{_level}%{data:message}",
"stdout": "%{data:proxysql_service}\\s+%{_revision}\\s+--\\s+%{notSpace:logger.name}\\s+--\\s+%{_textDate}",
"_longDate": "%{date(\"yyyy-MM-dd HH:mm:ss\"):date}",
"_textDate": "%{date(\"EEE MMM dd HH:mm:ss yyyy\"):date}",
"_level": "\\[%{word:level}\\]",
"_logger": "%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}\\(.*\\)",
"_revision": "rev.\\s+%{regex(\"\\\\d+.\\\\d+.\\\\d+\"):revision}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .duration, patterns: ["%{number:duration:scale(1000000)}ms","%{number:duration:scale(1000000000)}s","%{number:duration:scale(1000)}us","%{number:duration:scale(1)}ns"],
aliases: {
"duration_ms": "%{number:duration:scale(1000000)}ms",
"duration_s": "%{number:duration:scale(1000000000)}s",
"duration_us": "%{number:duration:scale(1000)}us",
"duration_ns": "%{number:duration:scale(1)}ns"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .client_addr, patterns: ["%{ipOrHost:network.client.ip}:%{number:network.client.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.client.ip}:%{number:network.client.port}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .proxy_addr, patterns: ["%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .extra_info, patterns: ["%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}"],
aliases: {
"logger": "%{notSpace:logger.name}\\:%{number:logger.line_number}\\:%{notSpace:logger.method_name}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
.custom.db.instance = .custom.schemaname
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
.custom.db.user = .custom.username
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
.custom.logger.thread_id = .custom.thread_id
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .client, patterns: ["%{ipOrHost:network.client.ip}:%{number:network.client.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.client.ip}:%{number:network.client.port}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.duration_us * 1000
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
.custom.db.statement = .custom.query
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
.custom.db.rows_affected = .custom.rows_affected
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
.custom.db.rows_sent = .custom.rows_sent
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .server, patterns: ["%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"],
aliases: {
"ip_and_port": "%{ipOrHost:network.proxysql.ip}:%{number:network.proxysql.port}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
} else if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
} else if exists(.custom.endtime_timestamp_us) {
.timestamp = .custom.endtime_timestamp_us
}
'''

[[transforms.processing.logs.pipelines.proxysql2.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[transforms.processing.logs.pipelines.azure]
name = "azure"
filter.type = "datadog_search"
filter.source = "source:(azure OR azure.alertsmanagement OR azure.analysisservices OR azure.apiconfiguration OR azure.apimanagement OR azure.authorization OR azure.automation OR azure.batchai OR azure.batchazure.cache OR azure.blockchain OR azure.cache OR azure.cdn OR azure.classiccompute OR azure.classicstorage OR azure.cognitiveservices OR azure.containerinstance OR azure.containerregistry OR azure.containerservice OR azure.datafactory OR azure.datalakestore OR azure.dbformariadb OR azure.dbformysql OR azure.dbforpostgresql OR azure.devices OR azure.documentdb OR azure.enterpriseknowledgegraph OR azure.eventgrid OR azure.eventhub OR azure.hdinsight OR azure.insights OR azure.iotcentral OR azure.keyvault OR azure.kusto OR azure.logic OR azure.machinelearningservices OR azure.managedidentity OR azure.operationalinsights OR azure.operationsmanagement OR azure.peering OR azure.relay OR azure.resourcegroup OR azure.resources OR azure.search OR azure.security OR azure.servicebus OR azure.servicefabric OR azure.streamanalytics OR azure.subscription OR azure.synapse)"

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
.custom.evt.category = .custom.category
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
.custom.evt.name = .custom.operationName
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
.custom.evt.outcome = .custom.resultType
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.callerIpAddress
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.durationMs * 1000000
'''

[[transforms.processing.logs.pipelines.azure.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.identity.authorization.evidence.principalId
'''

[transforms.processing.logs.pipelines.azure_web]
name = "azure_web"
filter.type = "datadog_search"
filter.source = "source:azure.web"

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
.custom.evt.category = .custom.category
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
.custom.evt.name = .custom.operationName
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
.custom.evt.outcome = .custom.resultType
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.callerIpAddress
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.durationMs * 1000000
'''

[[transforms.processing.logs.pipelines.azure_web.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.identity.authorization.evidence.principalId
'''

[transforms.processing.logs.pipelines.azure_storage]
name = "azure_storage"
filter.type = "datadog_search"
filter.source = "source:azure.storage"

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
.custom.evt.category = .custom.category
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
.custom.evt.name = .custom.operationName
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
.custom.evt.outcome = .custom.resultType
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.callerIpAddress
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.durationMs * 1000000
'''

[[transforms.processing.logs.pipelines.azure_storage.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.identity.authorization.evidence.principalId
'''

[transforms.processing.logs.pipelines.azure_network]
name = "azure_network"
filter.type = "datadog_search"
filter.source = "source:azure.network"

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
if exists(.custom.properties.eventProperties.title) {
.message = .custom.properties.eventProperties.title
}
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
.custom.evt.category = .custom.category
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
.custom.evt.name = .custom.operationName
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
.custom.evt.outcome = .custom.resultType
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.callerIpAddress
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.durationMs * 1000000
'''

[[transforms.processing.logs.pipelines.azure_network.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.identity.authorization.evidence.principalId
'''

[transforms.processing.logs.pipelines.azure_compute]
name = "azure_compute"
filter.type = "datadog_search"
filter.source = "source:azure.compute"

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
if exists(.custom.properties.eventProperties.title) {
.message = .custom.properties.eventProperties.title
}
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
.custom.evt.category = .custom.category
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
.custom.evt.name = .custom.operationName
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
.custom.evt.outcome = .custom.resultType
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.callerIpAddress
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.durationMs * 1000000
'''

[[transforms.processing.logs.pipelines.azure_compute.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.identity.authorization.evidence.principalId
'''

[transforms.processing.logs.pipelines.kubecost_cost_analyzer]
name = "kubecost_cost_analyzer"
filter.type = "datadog_search"
filter.source = "service:cost-analyzer"

[[transforms.processing.logs.pipelines.kubecost_cost_analyzer.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)","%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*","%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, )?","%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:thread_id} %{word:app.name}.go:%{number:lineno}] %{data:content}","%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}"],
aliases: {
"access.common": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
"access.combined": "%{access.common} (%{number:duration:scale(1000000000)} )?\"%{_referer}\" \"%{_user_agent}\"( \"%{_x_forwarded_for}\")?.*",
"error.format": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date_access} \\[%{word:level}\\] %{data:error.message}(, )?",
"RULE_TRACE": "%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}",
"RULE_TRACE_DOTGO": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}",
"RULE_NON_TRACES": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:thread_id} %{word:app.name}.go:%{number:lineno}] %{data:content}",
"RULE_LEAST_SPECIFIC": "%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}",
"_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}",
"_x_forwarded_for": "%{regex(\"[^\\\\\\\"]*\"):http._x_forwarded_for:nullIf(\"-\")}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kubecost_cost_analyzer.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"level:E") {
.custom.level = "ERR"
} else if match_datadog_query(.,"level:I") {
.custom.level = "INFO"
}

'''

[[transforms.processing.logs.pipelines.kubecost_cost_analyzer.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.twistlock_defender]
name = "twistlock_defender"
filter.type = "datadog_search"
filter.source = "source:defender"

[[transforms.processing.logs.pipelines.twistlock_defender.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[31m\\s+%{word:level}\\s+%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):date}.*"],
aliases: {
"autoFilledRule1": "\\[31m\\s+%{word:level}\\s+%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):date}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.twistlock_defender.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.wavefront_proxy]
name = "wavefront_proxy"
filter.type = "datadog_search"
filter.source = "service:wavefront-proxy"

[[transforms.processing.logs.pipelines.wavefront_proxy.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"yyyy-MM-dd HH:mm:ss','SSS\"):timestamp} %{word:level}  \\[%{word:pkg}:%{word:function}] \\[%{integer:maybe_thread_id}] %{data:message}","%{date(\"yyyy-MM-dd HH:mm:ss','SSS\"):timestamp} %{word:level}  \\[%{word:pkg}:%{word:function}] %{data:message}","%{date(\"yyyy-MM-dd HH:mm:ss','SSS\"):timestamp} %{word:level} \\[%{word:pkg}:%{word:function}] %{data:message}"],
aliases: {
"INFO": "%{date(\"yyyy-MM-dd HH:mm:ss','SSS\"):timestamp} %{word:level}  \\[%{word:pkg}:%{word:function}] \\[%{integer:maybe_thread_id}] %{data:message}",
"WARN": "%{date(\"yyyy-MM-dd HH:mm:ss','SSS\"):timestamp} %{word:level}  \\[%{word:pkg}:%{word:function}] %{data:message}",
"ERROR": "%{date(\"yyyy-MM-dd HH:mm:ss','SSS\"):timestamp} %{word:level} \\[%{word:pkg}:%{word:function}] %{data:message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.wavefront_proxy.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.aws_node_termination_handler]
name = "aws_node_termination_handler"
filter.type = "datadog_search"
filter.source = "service:aws-node-termination-handler"

[[transforms.processing.logs.pipelines.aws_node_termination_handler.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["","",""],
aliases: {
"rule": "",
"rule1": "",
"rule2": ""
})
.custom, err = merge(.custom, custom)
'''

[transforms.processing.logs.pipelines.etcd]
name = "etcd"
filter.type = "datadog_search"
filter.source = "source:etcd"

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_date} %{word: level} \\| (%{notSpace:package}: )?%{data:message}"],
aliases: {
"etcd": "%{_date} %{word: level} \\| (%{notSpace:package}: )?%{data:message}",
"_date": "%{date(\"yyyy-MM-dd HH:mm:ss.SSSSSS\"):date}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
}
'''

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
}
'''

[[transforms.processing.logs.pipelines.etcd.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.flagship_workers__kafka___rabbit_]
name = "flagship_workers__kafka___rabbit_"
filter.type = "datadog_search"
filter.source = "service:worker*"

[[transforms.processing.logs.pipelines.flagship_workers__kafka___rabbit_.transforms]]
type = "remap"
source = '''
.custom.async.retry_count = .custom.previous_attempts
'''

[transforms.processing.logs.pipelines.atlantis]
name = "atlantis"
filter.type = "datadog_search"
filter.source = "source:atlantis"

[[transforms.processing.logs.pipelines.atlantis.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{date(\"yyyy/MM/dd HH:mm:ssZ\"):date}\\s+\\[%{word:status}\\]\\s+%{data}"],
aliases: {
"autoFilledRule1": "%{date(\"yyyy/MM/dd HH:mm:ssZ\"):date}\\s+\\[%{word:status}\\]\\s+%{data}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.atlantis.transforms]]
type = "remap"
source = '''
status = string(.custom.status) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.source_oauth2_proxy]
name = "source_oauth2_proxy"
filter.type = "datadog_search"
filter.source = "source:oauth2_proxy"

[[transforms.processing.logs.pipelines.source_oauth2_proxy.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{ip:internal_ip} - %{notSpace:user} \\[%{date(\"yyyy/MM/dd HH:mm:ss\"):logged_date}\\] %{notSpace:hostname} %{notSpace:http.method} %{notSpace:internalHostPort} \\\"%{regex(\".*?\"):http.url_details.path}\\\" %{notSpace:http.protocol} \\\"%{regex(\".*?\"):http.useragent}\\\" %{number:http.status_code} %{number:http.response_bytes} %{number:duration}.*"],
aliases: {
"oauth_access_logs": "%{ip:internal_ip} - %{notSpace:user} \\[%{date(\"yyyy/MM/dd HH:mm:ss\"):logged_date}\\] %{notSpace:hostname} %{notSpace:http.method} %{notSpace:internalHostPort} \\\"%{regex(\".*?\"):http.url_details.path}\\\" %{notSpace:http.protocol} \\\"%{regex(\".*?\"):http.useragent}\\\" %{number:http.status_code} %{number:http.response_bytes} %{number:duration}.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.source_oauth2_proxy.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.log_status = "info"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.log_status = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.log_status = "warn"
} else if match_datadog_query(.,"@http.status_code:[500 TO 999]") {
.custom.log_status = "error"
}

'''

[[transforms.processing.logs.pipelines.source_oauth2_proxy.transforms]]
type = "remap"
source = '''
status = string(.custom.log_status) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.glog_pipeline]
name = "glog_pipeline"
filter.type = "datadog_search"
filter.source = "source:(admission-webhook OR api-server OR cert-manager-acmesolver OR cert-manager-cainjector OR cert-manager-controller OR cert-manager-webhook OR cluster-proportional-autoscaler-amd64 OR hyperkube OR ip-masq-agent OR k8s-prometheus-adapter-amd64 OR kube-apiserver OR kube-controller-manager OR kube-proxy OR kube-state-metrics OR metacontroller OR metrics-server-amd64 OR prometheus-operator OR vpa-admission-controller OR vpa-recommender OR vpa-updater)"

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"[IWEF]\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"glog_rule": "%{regex(\"[IWEF]\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.glog_pipeline.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.draino]
name = "draino"
filter.type = "datadog_search"
filter.source = "service:draino"

[[transforms.processing.logs.pipelines.draino.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}","%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:thread_id} %{word:app.name}.go:%{number:lineno}] %{data:content}","%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}","%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"):timestamp}\\s+%{word:level}\\s+%{word:logger.name}\\/%{notSpace:logger.filename}:%{number:logger.lineno}\\s+%{data:message}","%{word:level}\\s+ %{number:thread_id}\\s+  %{number:line_number}:\\s+%{data:message}"],
aliases: {
"RULE_TRACE": "%{regex(\"Trace\"):level}\\[%{number:trace.id}]:%{data:content}",
"RULE_TRACE_DOTGO": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:random_number} %{word:app.name}.go:%{number:lineno}] %{regex(\"Trace\"):trace}\\[%{number:trace.id}]: %{data:content}",
"RULE_NON_TRACES": "%{regex(\"[A-Z]\"):level}%{regex(\"[0-9]+\"):date} %{date(\"HH:mm:ss.SSSSSS\"):timestamp}%{space}%{number:thread_id} %{word:app.name}.go:%{number:lineno}] %{data:content}",
"RULE_LEAST_SPECIFIC": "%{word:level} %{date(\"HH:mm:ss.SSSSSS\"):date} %{data:content}",
"DRAINO": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS'Z'\"):timestamp}\\s+%{word:level}\\s+%{word:logger.name}\\/%{notSpace:logger.filename}:%{number:logger.lineno}\\s+%{data:message}",
"TEST": "%{word:level}\\s+ %{number:thread_id}\\s+  %{number:line_number}:\\s+%{data:message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.draino.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.better_coredns]
name = "better_coredns"
filter.type = "datadog_search"
filter.source = "service:coredns"

[[transforms.processing.logs.pipelines.better_coredns.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSZ\"):timestamp}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp})?\\s*\\[%{word:level}\\] %{notSpace:network.client.ip}:%{port:network.client.port} - %{number:dns.id} \"%{word:dns.question.type} %{word:dns.question.class} %{hostname:dns.question.name} %{word:dns.protocol} %{number:dns.question.size} %{word:dns.dnssec} %{number:dns.buffer}\" %{word:dns.flags.rcode}  %{number:dns.answer.size} %{number:duration:scale(1000000000)}s.*","(%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSZ\"):timestamp}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp})?\\s*\\[%{word:level}\\] %{notSpace:plugin}: %{data:message}","(%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSZ\"):timestamp}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp})?\\s*\\[%{word:level}\\] %{data:message}"],
aliases: {
"dns_common_format": "(%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSZ\"):timestamp}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp})?\\s*\\[%{word:level}\\] %{notSpace:network.client.ip}:%{port:network.client.port} - %{number:dns.id} \"%{word:dns.question.type} %{word:dns.question.class} %{hostname:dns.question.name} %{word:dns.protocol} %{number:dns.question.size} %{word:dns.dnssec} %{number:dns.buffer}\" %{word:dns.flags.rcode}  %{number:dns.answer.size} %{number:duration:scale(1000000000)}s.*",
"plugins": "(%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSZ\"):timestamp}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp})?\\s*\\[%{word:level}\\] %{notSpace:plugin}: %{data:message}",
"not_plugins": "(%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSZ\"):timestamp}|%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):timestamp})?\\s*\\[%{word:level}\\] %{data:message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.better_coredns.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.better_coredns.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[transforms.processing.logs.pipelines.voice_call_state_manager]
name = "voice_call_state_manager"
filter.type = "datadog_search"
filter.source = "@app.name:callstatemanager"

[[transforms.processing.logs.pipelines.voice_call_state_manager.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.voice_call_state_manager.transforms.logs]
order = [
    "call_ended"
]

[transforms.processing.logs.pipelines.voice_call_state_manager.transforms.logs.pipelines.call_ended]
name = "call_ended"
filter.type = "datadog_search"
filter.source = "\"Report call\""

[[transforms.processing.logs.pipelines.voice_call_state_manager.transforms.logs.pipelines.call_ended.transforms]]
type = "remap"
source = '''
.custom.event_duration_ms,err = .custom.voice.call.duration * 1000
'''


[transforms.processing.logs.pipelines.auth0]
name = "auth0"
filter.type = "datadog_search"
filter.source = "source:auth0"

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.data.date) {
.timestamp = .custom.data.date
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.data.ip
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
#TODO: geo-ip-parser
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
.custom.http.useragent = .custom.data.user_agent
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.http.useragent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.http.useragent)
}
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.data.user_name
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
.custom.usr.name = .custom.data.user_name
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
.custom.usr.email = .custom.data.details.request.auth.user.email
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
#TODO: lookup-processor
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
#TODO: lookup-processor
'''

[[transforms.processing.logs.pipelines.auth0.transforms]]
type = "remap"
source = '''
if exists(.custom.message) {
.message = .custom.message
} else if exists(.custom.data.description) {
.message = .custom.data.description
} else if exists(.custom.evt.name) {
.message = .custom.evt.name
}
'''

[transforms.processing.logs.pipelines.twiliowebhook]
name = "twiliowebhook"
filter.type = "datadog_search"
filter.source = "@app.name:twiliowebhook"

[[transforms.processing.logs.pipelines.twiliowebhook.transforms]]
type = "pipelines"

[transforms.processing.logs.pipelines.twiliowebhook.transforms.logs]
order = [
    "call_status_callback"
]

[transforms.processing.logs.pipelines.twiliowebhook.transforms.logs.pipelines.call_status_callback]
name = "call_status_callback"
filter.type = "datadog_search"
filter.source = "\"Receive call status\" OR \"Receive conference status\" OR \"Receive recording status\""

[[transforms.processing.logs.pipelines.twiliowebhook.transforms.logs.pipelines.call_status_callback.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .twilio.params, patterns: ["%{data:twilio.params:json}"],
aliases: {
"rule": "%{data:twilio.params:json}"
})
.custom, err = merge(.custom, custom)
'''


[transforms.processing.logs.pipelines.kube_scheduler__glog_]
name = "kube_scheduler__glog_"
filter.type = "datadog_search"
filter.source = "source:(kube_scheduler OR kube-scheduler)"

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"],
aliases: {
"kube_scheduler": "%{regex(\"\\\\w\"):level}%{date(\"MMdd HH:mm:ss.SSSSSS\"):timestamp}\\s+%{number:logger.thread_id} %{notSpace:logger.name}:%{number:lineno}\\] %{data:msg}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
if exists(.custom.timestamp) {
.timestamp = .custom.timestamp
}
'''

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.kube_scheduler__glog_.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.aws_ecs_agent]
name = "aws_ecs_agent"
filter.type = "datadog_search"
filter.source = "source:amazon-ecs-agent"

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}

'''

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.aws_ecs_agent.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[transforms.processing.logs.pipelines.coffeeapi]
name = "coffeeapi"
filter.type = "datadog_search"
filter.source = "service:coffee-api"

[[transforms.processing.logs.pipelines.coffeeapi.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.nodejs]
name = "nodejs"
filter.type = "datadog_search"
filter.source = "source:nodejs"

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.msg) {
.message = .custom.msg
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(?>\\[dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):time}\\]\\s+\\[%{word:level}\\] ","%{word:http.method} %{notSpace:http.url} %{number:http.status_code} (-|%{number:network.bytes_written}) - (%{number:duration:scale(1000000)} ms|%{number:duration:scale(1000000000)} s)","%{ipOrHost:network.client.ip} %{notSpace:http.ident:nullIf(\"-\")} %{notSpace:http.auth:nullIf(\"-\")} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}\\] \"(?>%{word:http.method} |)%{notSpace:http.url}(?> HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}|)\" %{number:http.status_code} (?>%{number:network.bytes_written}|-) \"%{notSpace:http.referer}\" \"%{regex(\"[^\\\\\\\"]*\"):http.useragent}\".*","(?>\\[dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?%{data::json}"],
aliases: {
"log4js_format": "(?>\\[dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?\\[%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSS\"):time}\\]\\s+\\[%{word:level}\\] ",
"web_access_morgan": "%{word:http.method} %{notSpace:http.url} %{number:http.status_code} (-|%{number:network.bytes_written}) - (%{number:duration:scale(1000000)} ms|%{number:duration:scale(1000000000)} s)",
"morgan_combined": "%{ipOrHost:network.client.ip} %{notSpace:http.ident:nullIf(\"-\")} %{notSpace:http.auth:nullIf(\"-\")} \\[%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}\\] \"(?>%{word:http.method} |)%{notSpace:http.url}(?> HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}|)\" %{number:http.status_code} (?>%{number:network.bytes_written}|-) \"%{notSpace:http.referer}\" \"%{regex(\"[^\\\\\\\"]*\"):http.useragent}\".*",
"fallback": "(?>\\[dd.trace_id=%{word:dd.trace_id} dd.span_id=%{word:dd.span_id}\\]\\s*)?%{data::json}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}

'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@level:10") {
.custom.bunyan_level = "trace"
} else if match_datadog_query(.,"@level:20") {
.custom.bunyan_level = "debug"
} else if match_datadog_query(.,"@level:30") {
.custom.bunyan_level = "info"
} else if match_datadog_query(.,"@level:40") {
.custom.bunyan_level = "warning"
} else if match_datadog_query(.,"@level:50") {
.custom.bunyan_level = "error"
} else if match_datadog_query(.,"@level:60") {
.custom.bunyan_level = "fatal"
}

'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? string(.custom.bunyan_level) ?? string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.http.url) {
.custom.http.url_details, err = parse_url(.custom.http.url)
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.http.useragent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.http.useragent)
}
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
.custom.env = .custom.dd.env
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
.custom.version = .custom.dd.version
'''

[[transforms.processing.logs.pipelines.nodejs.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
}
'''

[transforms.processing.logs.pipelines.postgresql]
name = "postgresql"
filter.type = "datadog_search"
filter.source = "source:postgresql"

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_prefix} %{_severity}:\\s+duration:\\s+%{_duration}\\s+ms\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})","%{_prefix} %{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})","(%{_timestamp}|%{_timestamp_ms} \\[%{_proc_id}])\\s+%{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})"],
aliases: {
"suggested_format_with_duration": "%{_prefix} %{_severity}:\\s+duration:\\s+%{_duration}\\s+ms\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
"suggested_format": "%{_prefix} %{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
"default_format": "(%{_timestamp}|%{_timestamp_ms} \\[%{_proc_id}])\\s+%{_severity}:\\s+(%{regex(\"statement:\")}\\s+%{_raw_query}|%{data:msg})",
"_timestamp": "%{date(\"yyyy-MM-dd HH:mm:ss z\"):db.date}",
"_timestamp_ms": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS z\"):db.date}",
"_database": "%{notSpace:db.instance}",
"_raw_query": "%{data:db.statement}",
"_duration": "%{numberExt:duration}",
"_severity": "%{notSpace:db.severity}",
"_user": "%{notSpace:db.user}",
"_client_ip": "%{notSpace:network.client.ip}",
"_proc_id": "%{notSpace:postgres.proc_id}",
"_session_id": "%{notSpace:postgres.session_id}",
"_app": "%{notSpace:postgres.appname}",
"_prefix": "%{_timestamp_ms} \\[%{_proc_id}\\] %{_database} %{_app} %{_user} %{_client_ip} %{_session_id}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .db.statement, patterns: ["%{word:db.operation} .*"],
aliases: {
"extract_operation": "%{word:db.operation} .*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
.timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
.custom.db = .custom.db.instance
'''

[[transforms.processing.logs.pipelines.postgresql.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.duration * 1000000
'''

[transforms.processing.logs.pipelines.transcription_frontdoor]
name = "transcription_frontdoor"
filter.type = "datadog_search"
filter.source = "service:transcription-frontdoor"

[[transforms.processing.logs.pipelines.transcription_frontdoor.transforms]]
type = "remap"
source = '''
.custom.endpoint = .custom.http.url_details.endpoint
'''

[[transforms.processing.logs.pipelines.transcription_frontdoor.transforms]]
type = "remap"
source = '''
.custom.httpstatus = .custom.http.status_code
'''

[transforms.processing.logs.pipelines.auth_logs___syslog_]
name = "auth_logs___syslog_"
filter.type = "datadog_search"
filter.source = "service:auth source:syslog"

[[transforms.processing.logs.pipelines.auth_logs___syslog_.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["(%{date(\"MMM dd HH:mm:ss\"):date}|%{date(\"MMM  d HH:mm:ss\"):date}) %{host:host} %{word:program}%{regex(\":* *\")}%{regex(\".*\"):sub_message}"],
aliases: {
"session_start": "(%{date(\"MMM dd HH:mm:ss\"):date}|%{date(\"MMM  d HH:mm:ss\"):date}) %{host:host} %{word:program}%{regex(\":* *\")}%{regex(\".*\"):sub_message}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.auth_logs___syslog_.transforms]]
type = "remap"
source = '''
if exists(.custom.sub_message) {
.message = .custom.sub_message
}
'''

[transforms.processing.logs.pipelines.orgservice]
name = "orgservice"
filter.type = "datadog_search"
filter.source = "service:orgservice"

[[transforms.processing.logs.pipelines.orgservice.transforms]]
type = "remap"
source = '''
.custom.or.org.shortname = .custom.short_name
'''

[transforms.processing.logs.pipelines.cassandra]
name = "cassandra"
filter.type = "datadog_search"
filter.source = "source:cassandra"

[[transforms.processing.logs.pipelines.cassandra.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_prefix} %{regex(\"Compacting\"):db.operation}.* %{_keyspace}\\/%{_table}:%{data:partition_key} \\(%{_bytes} bytes\\)","%{_prefix} %{regex(\"Flushing\"):db.operation}.*\\(Keyspace='%{_keyspace}', ColumnFamily='%{_table}'\\) %{data}: %{_onheap_total}\\/%{_offheap_total}, live: %{_onheap_live}\\/%{_offheap_live}, flushing: %{_onheap_flush}\\/%{_offheap_flush}, this: %{_onheap_this}\\/%{_offheap_this}","%{_prefix} %{regex(\"Enqueuing\"):db.operation}.* of %{_keyspace}: %{_onheap_bytes}%{data} \\(%{_onheap_pct}%\\) on-heap, %{_offheap_bytes} \\(%{_offheap_pct}%\\).*","%{_prefix} %{regex(\"Writing\"):db.operation}.*-%{_keyspace}%{data}\\(%{number:cassandra.bytes:scale(1000000)}%{data}, %{integer:cassandra.ops} ops, %{_onheap_pct}%\\/%{_offheap_pct}.*","%{_prefix} Completed %{regex(\"flushing\"):db.operation} %{_sstable} \\(%{number:cassandra.bytes_kb}KiB\\) for commitlog %{data:commitlog}","%{_prefix}\\s+%{regex(\"Compacted\"):db.operation}.* to \\[%{_sstable}\\].\\s+%{notSpace:cassandra.bytes_in} bytes to %{notSpace:cassandra.bytes_out} \\(\\~%{integer:cassandra.percent_of_orig}% of original\\) in %{notSpace:cassandra.duration_ms}ms = %{number:cassandra.speed_mb}MB/s.\\s+%{notSpace:cassandra.pkeys_in} total partitions merged to %{notSpace:cassandra.pkeys_out}\\.\\s+Partition merge counts were %{data:cassandra.merge_cnt}","%{_prefix} G.* %{integer:duration:scale(1000000)}ms. %{data}: %{integer:cassandra.eden.orig_bytes} -> %{integer:cassandra.eden.new_bytes}; %{data}: %{integer:cassandra.oldgen.orig_bytes} -> %{integer:cassandra.oldgen.new_bytes};.*","%{_prefix} %{word:cassandra.pool}\\s*(?>%{integer:cassandra.cache_used}\\s*%{integer:cassandra.cache_size}\\s*all|%{integer:cassandra.threads.active}\\s*%{integer:cassandra.threads.pending}\\s*%{integer:cassandra.threads.completed}\\s*%{integer:cassandra.threads.blocked}\\s*%{integer:cassandra.threads.all_time_blocked}|%{integer:cassandra.threads.active}\\s*%{integer:cassanadra.threads.pending})","%{_prefix} %{integer:db.operations} operations were slow in the last %{integer:elapsed_time:scale(1000000)} msecs:\\n","%{_prefix} %{data:msg}"],
aliases: {
"cassandra_compaction_key": "%{_prefix} %{regex(\"Compacting\"):db.operation}.* %{_keyspace}\\/%{_table}:%{data:partition_key} \\(%{_bytes} bytes\\)",
"cassandra_pool_cleaner": "%{_prefix} %{regex(\"Flushing\"):db.operation}.*\\(Keyspace='%{_keyspace}', ColumnFamily='%{_table}'\\) %{data}: %{_onheap_total}\\/%{_offheap_total}, live: %{_onheap_live}\\/%{_offheap_live}, flushing: %{_onheap_flush}\\/%{_offheap_flush}, this: %{_onheap_this}\\/%{_offheap_this}",
"cassandra_pool_cleaner2": "%{_prefix} %{regex(\"Enqueuing\"):db.operation}.* of %{_keyspace}: %{_onheap_bytes}%{data} \\(%{_onheap_pct}%\\) on-heap, %{_offheap_bytes} \\(%{_offheap_pct}%\\).*",
"cassandra_table_flush": "%{_prefix} %{regex(\"Writing\"):db.operation}.*-%{_keyspace}%{data}\\(%{number:cassandra.bytes:scale(1000000)}%{data}, %{integer:cassandra.ops} ops, %{_onheap_pct}%\\/%{_offheap_pct}.*",
"cassandra_mem_flush": "%{_prefix} Completed %{regex(\"flushing\"):db.operation} %{_sstable} \\(%{number:cassandra.bytes_kb}KiB\\) for commitlog %{data:commitlog}",
"cassandra_compaction": "%{_prefix}\\s+%{regex(\"Compacted\"):db.operation}.* to \\[%{_sstable}\\].\\s+%{notSpace:cassandra.bytes_in} bytes to %{notSpace:cassandra.bytes_out} \\(\\~%{integer:cassandra.percent_of_orig}% of original\\) in %{notSpace:cassandra.duration_ms}ms = %{number:cassandra.speed_mb}MB/s.\\s+%{notSpace:cassandra.pkeys_in} total partitions merged to %{notSpace:cassandra.pkeys_out}\\.\\s+Partition merge counts were %{data:cassandra.merge_cnt}",
"cassandra_gc_format": "%{_prefix} G.* %{integer:duration:scale(1000000)}ms. %{data}: %{integer:cassandra.eden.orig_bytes} -> %{integer:cassandra.eden.new_bytes}; %{data}: %{integer:cassandra.oldgen.orig_bytes} -> %{integer:cassandra.oldgen.new_bytes};.*",
"cassandra_thread_pending": "%{_prefix} %{word:cassandra.pool}\\s*(?>%{integer:cassandra.cache_used}\\s*%{integer:cassandra.cache_size}\\s*all|%{integer:cassandra.threads.active}\\s*%{integer:cassandra.threads.pending}\\s*%{integer:cassandra.threads.completed}\\s*%{integer:cassandra.threads.blocked}\\s*%{integer:cassandra.threads.all_time_blocked}|%{integer:cassandra.threads.active}\\s*%{integer:cassanadra.threads.pending})",
"cassandra_slow_statements": "%{_prefix} %{integer:db.operations} operations were slow in the last %{integer:elapsed_time:scale(1000000)} msecs:\\n",
"cassandra_fallback_parser": "%{_prefix} %{data:msg}",
"_level": "%{word:db.severity}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_thread_id": "%{integer:logger.thread_id}",
"_logger_name": "%{notSpace:logger.name}",
"_table": "%{word:db.table}",
"_sstable": "%{notSpace:cassandra.sstable}",
"_bytes": "%{integer:cassandra.bytes}",
"_keyspace": "%{word:cassandra.keyspace}",
"_onheap_total": "%{number:cassandra.onheap.total}",
"_onheap_live": "%{number:cassandra.onheap.live}",
"_onheap_flush": "%{number:cassandra.onheap.flush}",
"_onheap_this": "%{number:cassandra.onheap.this}",
"_onheap_bytes": "%{integer:cassandra.onheap.bytes}",
"_onheap_pct": "%{integer:cassandra.onheap.percent}",
"_offheap_total": "%{number:cassandra.offheap.total}",
"_offheap_live": "%{number:cassandra.offheap.live}",
"_offheap_flush": "%{number:cassandra.offheap.flush}",
"_offheap_this": "%{number:cassandra.offheap.this}",
"_offheap_bytes": "%{integer:cassandra.offheap.bytes}",
"_offheap_pct": "%{integer:cassandra.offheap.percent}",
"_default_prefix": "%{_level}\\s+\\[(%{_thread_name}:%{_thread_id}|%{_thread_name})\\]\\s+%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):db.date}\\s+%{word:filename}.java:%{integer:lineno} -",
"_suggested_prefix": "%{date(\"yyyy-MM-dd HH:mm:ss\"):db.date} \\[(%{_thread_name}:%{_thread_id}|%{_thread_name})\\] %{_level} %{_logger_name}\\s+-",
"_prefix": "(?>%{_default_prefix}|%{_suggested_prefix})"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.cassandra.transforms]]
type = "remap"
source = '''
if exists(.custom.db.date) {
.timestamp = .custom.db.date
}
'''

[[transforms.processing.logs.pipelines.cassandra.transforms]]
type = "remap"
source = '''
status = string(.custom.db.severity) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.public_calendar]
name = "public_calendar"
filter.type = "datadog_search"
filter.source = "@product.host:public-calendar"

[[transforms.processing.logs.pipelines.public_calendar.transforms]]
type = "remap"
source = '''
.custom.http.url = .custom.url
'''

[[transforms.processing.logs.pipelines.public_calendar.transforms]]
type = "remap"
source = '''
.custom.errorMessage = .custom.additionalData.errorMessage
'''

[transforms.processing.logs.pipelines.apache_httpd]
name = "apache_httpd"
filter.type = "datadog_search"
filter.source = "source:httpd"

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)","%{access.common} \"%{_referer}\" \"%{_user_agent}\""],
aliases: {
"access.common": "%{_client_ip} %{_ident} %{_auth} \\[%{_date_access}\\] \"(?>%{_method} |)%{_url}(?> %{_version}|)\" %{_status_code} (?>%{_bytes_written}|-)",
"access.combined": "%{access.common} \"%{_referer}\" \"%{_user_agent}\"",
"_auth": "%{notSpace:http.auth:nullIf(\"-\")}",
"_bytes_written": "%{integer:network.bytes_written}",
"_client_ip": "%{ipOrHost:network.client.ip}",
"_version": "HTTP\\/%{regex(\"\\\\d+\\\\.\\\\d+\"):http.version}",
"_url": "%{notSpace:http.url}",
"_ident": "%{notSpace:http.ident:nullIf(\"-\")}",
"_user_agent": "%{regex(\"[^\\\\\\\"]*\"):http.useragent}",
"_referer": "%{notSpace:http.referer}",
"_status_code": "%{integer:http.status_code}",
"_method": "%{word:http.method}",
"_date_access": "%{date(\"dd/MMM/yyyy:HH:mm:ss Z\"):date_access}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if exists(.custom.http.useragent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.http.useragent)
}
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if exists(.custom.http.url) {
.custom.http.url_details, err = parse_url(.custom.http.url)
}
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if exists(.custom.date_access) {
.timestamp = .custom.date_access
}
'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
if match_datadog_query(.,"@http.status_code:[200 TO 299]") {
.custom.http.status_category = "OK"
} else if match_datadog_query(.,"@http.status_code:[300 TO 399]") {
.custom.http.status_category = "notice"
} else if match_datadog_query(.,"@http.status_code:[400 TO 499]") {
.custom.http.status_category = "warning"
} else if match_datadog_query(.,"@http.status_code:[500 TO 599]") {
.custom.http.status_category = "error"
}

'''

[[transforms.processing.logs.pipelines.apache_httpd.transforms]]
type = "remap"
source = '''
status = string(.custom.http.status_category) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.flubby]
name = "flubby"
filter.type = "datadog_search"
filter.source = "service:telefork image_name:\"gcr.io/outreach-docker/flubby\""

[[transforms.processing.logs.pipelines.flubby.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["\\[%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\]\\s+\\[\\s+%{word:severity}\\].*","\\[%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\]\\s+\\[+%{word:severity}\\].*","%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\s+\\[+%{word:severity}\\].*","%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\s+\\[flb-go-amplitude].*","^.*:.*$","^\\* .*","^\\'.*"],
aliases: {
"autoFilledRule1": "\\[%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\]\\s+\\[\\s+%{word:severity}\\].*",
"autoFilledRule2": "\\[%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\]\\s+\\[+%{word:severity}\\].*",
"autoFilledRule3": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\s+\\[+%{word:severity}\\].*",
"autoFilledRule4": "%{date(\"yyyy/MM/dd HH:mm:ss\"):date}\\s+\\[flb-go-amplitude].*",
"rule3": "^.*:.*$",
"rule5": "^\\* .*",
"rule6": "^\\'.*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.flubby.transforms]]
type = "remap"
source = '''
#TODO: lookup-processor
'''

[[transforms.processing.logs.pipelines.flubby.transforms]]
type = "remap"
source = '''
status = string(.custom.severity2) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[transforms.processing.logs.pipelines.voice_dialer]
name = "voice_dialer"
filter.type = "datadog_search"
filter.source = "@dd.service:ingestitron @product.host:flagshipVoice"

[[transforms.processing.logs.pipelines.voice_dialer.transforms]]
type = "remap"
source = '''
.custom.or.voice_trace_id = .custom.additionalData.callUid
'''

[transforms.processing.logs.pipelines.azure_recovery_services]
name = "azure_recovery_services"
filter.type = "datadog_search"
filter.source = "source:azure.recoveryservices"

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
if exists(.custom.time) {
.timestamp = .custom.time
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
.custom.evt.category = .custom.category
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
.custom.evt.name = .custom.operationName
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
.custom.evt.outcome = .custom.resultType
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
.custom.network.client.ip = .custom.callerIpAddress
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
.custom.duration,err = .custom.durationMs * 1000000
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
.custom.usr.id = .custom.identity.authorization.evidence.principalId
'''

[[transforms.processing.logs.pipelines.azure_recovery_services.transforms]]
type = "remap"
source = '''
.custom.entity_name = .custom.properties.Entity_Name
'''

[transforms.processing.logs.pipelines.argocd_application_controller]
name = "argocd_application_controller"
filter.type = "datadog_search"
filter.source = "service:argocd-application-controller source:argocd"

[[transforms.processing.logs.pipelines.argocd_application_controller.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["Updated\\s+health\\s+status\\:\\s+%{notSpace:fromHealth}\\s+-\\>\\s+%{notSpace:toHealth}"],
aliases: {
"UpdatedHealth": "Updated\\s+health\\s+status\\:\\s+%{notSpace:fromHealth}\\s+-\\>\\s+%{notSpace:toHealth}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.argocd_application_controller.transforms]]
type = "remap"
source = '''
.custom.argocd_application_name = .custom.application
'''

[[transforms.processing.logs.pipelines.argocd_application_controller.transforms]]
type = "remap"
source = '''
.custom.argocd_application_name = .custom.application
'''

[transforms.processing.logs.pipelines.c_]
name = "c_"
filter.type = "datadog_search"
filter.source = "source:csharp"

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
.custom.logger.name = .custom.logger
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
.custom.logger.thread_name = .custom.thread
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
.custom.error.stack = .custom.exception
.custom.error.stack = .custom.stack_trace
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .message, patterns: ["%{_date} %{_status} \\[%{_thread_name}\\] %{_logger_name} %{_method}:%{_line} - %{data:message}((\\n|\\t)%{data:error.stack})?","(%{_date}|%{_date_ms}) %{_status}\\s+%{data:message}((\\n|\\t)%{data:error.stack})?","%{date(\"yyyy-MM-dd HH:mm:ss.SSS Z\"):date} \\[%{_status}\\] ","(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSSSS\"):date})\\|%{_status}\\|%{notSpace:logger.name}\\|\\{\\}.*","%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):date} \\[%{number}\\] %{_status}\\s+%{notSpace:logger.name} \\[%{notSpace}\\] \\{\\}.*"],
aliases: {
"recommended_format": "%{_date} %{_status} \\[%{_thread_name}\\] %{_logger_name} %{_method}:%{_line} - %{data:message}((\\n|\\t)%{data:error.stack})?",
"default_parser": "(%{_date}|%{_date_ms}) %{_status}\\s+%{data:message}((\\n|\\t)%{data:error.stack})?",
"serilog_format": "%{date(\"yyyy-MM-dd HH:mm:ss.SSS Z\"):date} \\[%{_status}\\] ",
"Nlog_format": "(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSSSS\"):date})\\|%{_status}\\|%{notSpace:logger.name}\\|\\{\\}.*",
"log4net_format": "%{date(\"yyyy-MM-dd HH:mm:ss,SSS\"):date} \\[%{number}\\] %{_status}\\s+%{notSpace:logger.name} \\[%{notSpace}\\] \\{\\}.*",
"_date": "(%{date(\"yyyy-MM-dd HH:mm:ss.SSSS\"):date}|%{date(\"yyyy-MM-dd HH:mm:ss.SSS\"):date})",
"_date_ms": "%{date(\"yyyy-MM-dd'T'HH:mm:ss.SSSZ\"):date}",
"_status": "%{word:level}",
"_thread_name": "%{notSpace:logger.thread_name}",
"_logger_name": "%{notSpace:logger.name}",
"_line": "%{integer:line}",
"_method": "%{notSpace:method}"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.date) {
.timestamp = .custom.date
} else if exists(.custom.time) {
.timestamp = .custom.time
} else if exists(.custom.@t) {
.timestamp = .custom.@t
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
status = string(.custom.level) ?? string(.custom.Level) ?? string(.custom.@l) ?? null
if status == null {
 .status = 6
} else {
if starts_with(status, "e", case_sensitive: false) {
 .status = 0
} else if starts_with(status, "a", case_sensitive: false) {
 .status = 1
} else if starts_with(status, "c", case_sensitive: false) {
 .status = 2
} else if starts_with(status, "e", case_sensitive: false) {
 .status = 3
} else if starts_with(status, "w", case_sensitive: false) {
 .status = 4
} else if starts_with(status, "n", case_sensitive: false) {
 .status = 5
} else if starts_with(status, "i", case_sensitive: false) {
 .status = 6
} else if starts_with(status, "d", case_sensitive: false) {
 .status = 7
} else if starts_with(status, "o", case_sensitive: false) {
 .status = 8
}
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
custom, err = parse_groks(value: .error.stack, patterns: ["%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"],
aliases: {
"error_rule": "%{notSpace:error.kind}: %{data:error.message}(\\n|\\t).*"
})
.custom, err = merge(.custom, custom)
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.trace_id) {
.trace_id = .custom.dd.trace_id
} else if exists(.custom.dd_trace_id) {
.trace_id = .custom.dd_trace_id
} else if exists(.custom.Properties.dd.trace_id) {
.trace_id = .custom.Properties.dd.trace_id
} else if exists(.custom.properties.dd.trace_id) {
.trace_id = .custom.properties.dd.trace_id
} else if exists(.custom.Properties.dd_trace_id) {
.trace_id = .custom.Properties.dd_trace_id
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.RenderedMessage) {
.message = .custom.RenderedMessage
} else if exists(.custom.@m) {
.message = .custom.@m
} else if exists(.custom.@mt) {
.message = .custom.@mt
} else if exists(.custom.message) {
.message = .custom.message
}
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
.custom.env = .custom.dd.env
.custom.env = .custom.dd_env
.custom.env = .custom.Properties.dd.env
.custom.env = .custom.properties.dd.env
.custom.env = .custom.Properties.dd_env
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
.custom.version = .custom.dd.version
.custom.version = .custom.dd_version
.custom.version = .custom.Properties.dd.version
.custom.version = .custom.properties.dd.version
.custom.version = .custom.Properties.dd_version
'''

[[transforms.processing.logs.pipelines.c_.transforms]]
type = "remap"
source = '''
if exists(.custom.dd.service) {
.service = .custom.dd.service
} else if exists(.custom.dd_service) {
.service = .custom.dd_service
} else if exists(.custom.Properties.dd.service) {
.service = .custom.Properties.dd.service
} else if exists(.custom.properties.dd.service) {
.service = .custom.properties.dd.service
} else if exists(.custom.Properties.dd_service) {
.service = .custom.Properties.dd_service
}
'''

[transforms.processing.logs.pipelines.flagshiptriggers]
name = "flagshiptriggers"
filter.type = "datadog_search"
filter.source = "\"trigger result\" service:worker-trigger-processing"

[[transforms.processing.logs.pipelines.flagshiptriggers.transforms]]
type = "remap"
source = '''
.custom.trigger.id = .custom.trigger_id
'''

[[transforms.processing.logs.pipelines.flagshiptriggers.transforms]]
type = "remap"
source = '''
.custom.skipped = .custom.litmus_skipped
'''

[transforms.processing.logs.pipelines.litmustriggers]
name = "litmustriggers"
filter.type = "datadog_search"
filter.source = "\"trigger result\" service:litmus"

[[transforms.processing.logs.pipelines.litmustriggers.transforms]]
type = "remap"
source = '''
.custom.async.trigger_consumer_job_id = .custom.async.job_id
'''

[transforms.processing.logs.pipelines.trigger_entry_point]
name = "trigger_entry_point"
filter.type = "datadog_search"
filter.source = "service:(worker-trigger OR worker-delayed-trigger)"

[[transforms.processing.logs.pipelines.trigger_entry_point.transforms]]
type = "remap"
source = '''
.custom.async.trigger_consumer_job_id = .custom.async.job_id
'''

[transforms.processing.logs.pipelines.web_browser_logs]
name = "web_browser_logs"
filter.type = "datadog_search"
filter.source = "source:browser"

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if exists(.custom.http.url) {
.custom.http.url_details, err = parse_url(.custom.http.url)
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if exists(.custom.http.useragent) {
.custom.http.useragent_details,err = parse_user_agent(.custom.http.useragent)
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if exists(.custom.view.url) {
.custom.view.url_details, err = parse_url(.custom.view.url)
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
if exists(.custom.view.referrer) {
.custom.view.referrer_details, err = parse_url(.custom.view.referrer)
}
'''

[[transforms.processing.logs.pipelines.web_browser_logs.transforms]]
type = "remap"
source = '''
#TODO: geo-ip-parser
'''

[transforms.processing.logs.pipelines.voice_insights]
name = "voice_insights"
filter.type = "datadog_search"
filter.source = "@event_category:voice_insights"

[[transforms.processing.logs.pipelines.voice_insights.transforms]]
type = "remap"
source = '''
.custom.or.voice_trace_id = .custom.voice_trace_id
'''

[transforms.postprocessing]
type = "remap"
inputs = ["processing"]
source = '''
. = compact(.)
'''

# Print parsed logs to stdout
[sinks.print]
type = "console"
inputs = ["postprocessing"]
encoding.codec = "json"

# Vector's GraphQL API (disabled by default)
# Uncomment to try it out with the `vector top` command or
# in your browser at http://localhost:8686
#[api]
#enabled = true
#address = "127.0.0.1:8686"