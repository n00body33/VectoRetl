#!/usr/bin/env python3
import pandas as pd
import scipy.stats
import argparse
import math
import glob
import os

def human_bytes(b):
    is_negative = False
    if b < 0:
        is_negative = True
        b = -b
    if b < 1 and b >= 0:
        return "0B"
    names = ("B", "KiB", "MiB", "GiB", "TiB", "PiB", "EiB", "ZiB", "YiB")
    i = int(math.floor(math.log(b, 1024)))
    p = math.pow(1024, i)
    s = round(b / p, 2)
    if is_negative:
        s = -s
    return "%s %s" % (s, names[i])


parser = argparse.ArgumentParser(description='t-test experiments with Welch method')
parser.add_argument('--capture-dir', type=str, help='the directory to search for capture csv files')
parser.add_argument('--warmup-seconds', type=int, help='the number of seconds to treat as warmup')
parser.add_argument('--baseline-sha', type=str, help='the sha of the baseline experiment')
parser.add_argument('--comparison-sha', type=str, help='the sha of the comparison experiment')
parser.add_argument('--vector-cpus', type=int, help='the total number of CPUs given to vector during the experiment')
parser.add_argument('--p-value', type=float, default=0.05, help='the p-value for comparing with t-test results, the smaller the more certain')
args = parser.parse_args()

capture_paths = glob.glob(os.path.join(args.capture_dir, "**/*.captures"))
captures = []
for f in capture_paths:
    captures.append(pd.read_csv(f))
csv = pd.concat(captures)

fetch_index_past_warmup = csv['fetch_index'] > args.warmup_seconds
csv = csv[fetch_index_past_warmup]
csv['value'] = csv['value'].div(args.vector_cpus)

print("# Soak Test Results")
print("Baseline: {}".format(args.baseline_sha))
print("Comparison: {}".format(args.comparison_sha))
print("Total Vector CPUs: {}".format(args.vector_cpus))
print("")
print("What follows is a statistical summary of the soak captures between the")
print("SHAs given above. Units are bytes/second/CPU, except for 'skewness'. The")
print("further 'skewness' is from 0.0 the more indication that vector lacks")
print("consistency in behavior, making predictions of fitness in the field")
print("challenging.")
print("")

adjust_per_cpu = lambda b: b/args.vector_cpus

ttest_results = []
for exp in csv.experiment.unique():
    experiment = csv[csv['experiment'] == exp]

    baseline = experiment[experiment['variant'] == 'baseline']
    comparison = experiment[experiment['variant'] == 'comparison']
    baseline_mean = baseline['value'].mean()
    baseline_stdev = baseline['value'].std()
    comparison_mean = comparison['value'].mean()
    comparison_stdev = comparison['value'].std()
    diff =  comparison_mean - baseline_mean

    # The t-test here is calculating whether the expected mean of our two
    # distributions is equal, or, put another way, whether the samples we have
    # here are from identical distributions. The higher the returned p-value by
    # ttest_ind the more likely it is that the samples _do_ have the same
    # expected mean.
    #
    # If the p-value is below our threshold then it is _unlikely_ that the two
    # samples actually have the same mean -- are from the same distribution --
    # and so there's some statistically interesting difference between the two
    # samples. For our purposes here that implies that performance has changed.
    #
    # TODO we don't currently detect outliers in the samples, which can
    # contaminate the result here. Yuen's test is supported by this
    # function. Detect outliers using Tukey's method, then flip between Welch's
    # method and Yuen depending on outlier existence.
    res = scipy.stats.ttest_ind(baseline['value'], comparison['value'], equal_var=False)
    ttest_results.append({'experiment': exp, 'Δ mean': diff.mean(),
                          'baseline mean': baseline_mean,
                          'baseline stdev': baseline_stdev,
                          'comparison mean': comparison_mean,
                          'comparison stdev': comparison_stdev,
                          't-statistic': res.statistic, 'p-value': res.pvalue })

ttest_results = pd.DataFrame.from_records(ttest_results)
print("## Change Detection")
print("")
print("The following table lists those experiments that have experienced a")
print("statistically significant change in their throughput performance between")
print("baseline and comparision SHAs, with {}% confidence. Negative values mean".format((1.0 - args.p_value) * 100))
print("that baseline is faster, positive comparison.")
print("")

p_value_violation = ttest_results['p-value'] < args.p_value
summary_mean_diff = ttest_results[p_value_violation]
summary_mean_diff = summary_mean_diff.drop(labels=['t-statistic', 'p-value', 'baseline mean',
                                                   'baseline stdev', 'comparison mean',
                                                   'comparison stdev'], axis=1)
summary_mean_diff['Δ mean'] = summary_mean_diff['Δ mean'].apply(adjust_per_cpu).apply(human_bytes)
print(summary_mean_diff.to_markdown(index=False, tablefmt='github'))

print("")
print("<details>")
print("<summary>Fine details of change detection per experiment.</summary>")
print("")
ttest_results['Δ mean'] = ttest_results['Δ mean'].apply(adjust_per_cpu).apply(human_bytes)
ttest_results['baseline mean'] = ttest_results['baseline mean'].apply(adjust_per_cpu).apply(human_bytes)
ttest_results['baseline stdev'] = ttest_results['baseline stdev'].apply(adjust_per_cpu).apply(human_bytes)
ttest_results['comparison mean'] = ttest_results['comparison mean'].apply(adjust_per_cpu).apply(human_bytes)
ttest_results['comparison stdev'] = ttest_results['comparison stdev'].apply(adjust_per_cpu).apply(human_bytes)
print(ttest_results.to_markdown(index=False, tablefmt='github'))
print("")
print("</details>")

print("")
print("<details>")
print("<summary>Fine details of each soak run.</summary>")
print("")
describe = csv.groupby(['experiment', 'variant'])['value'].describe(percentiles=[0.90, 0.95, 0.99])
describe = describe.rename(columns={'50%': 'average', '95%': 'p95', '90%': 'p90', '99%': 'p99'})
describe['skewness'] = csv.groupby(['experiment', 'variant'])['value'].skew()
describe['mean'] = describe['mean'].apply(adjust_per_cpu).apply(human_bytes)
describe['std'] = describe['std'].apply(adjust_per_cpu).apply(human_bytes)
describe['min'] = describe['min'].apply(adjust_per_cpu).apply(human_bytes)
describe['average'] = describe['average'].apply(adjust_per_cpu).apply(human_bytes)
describe['p90'] = describe['p90'].apply(adjust_per_cpu).apply(human_bytes)
describe['p95'] = describe['p95'].apply(adjust_per_cpu).apply(human_bytes)
describe['p99'] = describe['p99'].apply(adjust_per_cpu).apply(human_bytes)
describe['max'] = describe['max'].apply(adjust_per_cpu).apply(human_bytes)
print(describe.to_markdown(index=True,
                           tablefmt='github',
                           headers=['(experiment, variant)', 'total samples',
                                    'mean', 'std', 'min', 'average',
                                    'p90', 'p95', 'p99', 'max', 'skewness']))
print("")
print("</details>")
